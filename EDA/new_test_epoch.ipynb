{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Notebook - for developing code \n",
    "## test epoch with extra padding \n",
    "test new functions with extra padding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "os.environ['MPLCONFIGDIR'] = '/myhome'\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "from data import BavarianCrops, BreizhCrops, SustainbenchCrops, ModisCDL\n",
    "from torch.utils.data import DataLoader\n",
    "from earlyrnn import EarlyRNN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from loss import EarlyRewardLoss\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from utils.plots import plot_label_distribution_datasets, boxplot_stopping_times\n",
    "from utils.doy import get_doys_dict_test, get_doy_stop, create_sorted_doys_dict_test, get_approximated_doys_dict\n",
    "from utils.helpers_training import parse_args, train_epoch, test_epoch\n",
    "from utils.metrics import harmonic_mean_score\n",
    "import matplotlib.pyplot as plt\n",
    "from models.model_helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Run ELECTS Early Classification training on the BavarianCrops dataset.')\n",
    "    parser.add_argument('--backbonemodel', type=str, default=\"LSTM\", choices=[\"LSTM\", \"TempCNN\", \"Transformer\"], help=\"backbone model\")\n",
    "    parser.add_argument('--dataset', type=str, default=\"bavariancrops\", choices=[\"bavariancrops\",\"breizhcrops\", \"ghana\", \"southsudan\",\"unitedstates\"], help=\"dataset\")\n",
    "    parser.add_argument('--alpha', type=float, default=0.5, help=\"trade-off parameter of earliness and accuracy (eq 6): \"\n",
    "                                                                 \"1=full weight on accuracy; 0=full weight on earliness\")\n",
    "    parser.add_argument('--epsilon', type=float, default=10, help=\"additive smoothing parameter that helps the \"\n",
    "                                                                  \"model recover from too early classifications (eq 7)\")\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-3, help=\"Optimizer learning rate\")\n",
    "    parser.add_argument('--weight-decay', type=float, default=0, help=\"weight_decay\")\n",
    "    parser.add_argument('--patience', type=int, default=30, help=\"Early stopping patience\")\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                        choices=[\"cuda\", \"cpu\"], help=\"'cuda' (GPU) or 'cpu' device to run the code. \"\n",
    "                                                     \"defaults to 'cuda' if GPU is available, otherwise 'cpu'\")\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"number of training epochs\")\n",
    "    parser.add_argument('--sequencelength', type=int, default=70, help=\"sequencelength of the time series. If samples are shorter, \"\n",
    "                                                                \"they are zero-padded until this length; \"\n",
    "                                                                \"if samples are longer, they will be undersampled\")\n",
    "    parser.add_argument('--hidden-dims', type=int, default=64, help=\"number of hidden dimensions in the backbone model\")\n",
    "    parser.add_argument('--batchsize', type=int, default=256, help=\"number of samples per batch\")\n",
    "    parser.add_argument('--dataroot', type=str, default=os.path.join(os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\")),\"elects_data\"), help=\"directory to download the \"\n",
    "                                                                                 \"BavarianCrops dataset (400MB).\"\n",
    "                                                                                 \"Defaults to home directory.\")\n",
    "    parser.add_argument('--snapshot', type=str, default=\"snapshots/model.pth\",\n",
    "                        help=\"pytorch state dict snapshot file\")\n",
    "    parser.add_argument('--resume', action='store_true')\n",
    "\n",
    "    if args is not None:\n",
    "        args = parser.parse_args(args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if args.patience < 0:\n",
    "        args.patience = None\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available:  cuda\n",
      "Namespace(backbonemodel='TempCNN', dataset='breizhcrops', alpha=0.5, epsilon=10, learning_rate=0.001, weight_decay=0, patience=30, device='cuda', epochs=10, sequencelength=70, hidden_dims=16, batchsize=256, dataroot='C:\\\\Users\\\\anyam\\\\elects_data', snapshot='snapshots/model.pth', resume=False)\n"
     ]
    }
   ],
   "source": [
    "# Example of how to call parse_args with custom arguments in a notebook\n",
    "custom_args = \"--backbonemodel TempCNN --dataset breizhcrops --epochs 10 --hidden-dims 16\".split()\n",
    "args = parse_args(custom_args)\n",
    "print(\"cuda is available: \", args.device)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: aurenore. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\wandb\\run-20240404_130152-fkmhkc1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aurenore/ELECTS/runs/fkmhkc1a/workspace' target=\"_blank\">charmed-lake-84</a></strong> to <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">https://wandb.ai/aurenore/ELECTS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aurenore/ELECTS/runs/fkmhkc1a/workspace' target=\"_blank\">https://wandb.ai/aurenore/ELECTS/runs/fkmhkc1a/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aurenore/ELECTS/runs/fkmhkc1a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x201b5a87850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/mydata/studentanya/anya/wandb/\"\n",
    "wandb.init(\n",
    "        dir=None,\n",
    "        project=\"ELECTS\",\n",
    "        notes=\"first experimentations with ELECTS\",\n",
    "        tags=[\"ELECTS\", args.dataset, \"with_doys_boxplot\", args.backbonemodel],\n",
    "        config={\n",
    "        \"backbonemodel\": args.backbonemodel,\n",
    "        \"dataset\": args.dataset,\n",
    "        \"alpha\": args.alpha,\n",
    "        \"epsilon\": args.epsilon,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "        \"patience\": args.patience,\n",
    "        \"device\": args.device,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"sequencelength\": args.sequencelength,\n",
    "        \"hidden_dims\": args.hidden_dims,\n",
    "        \"batchsize\": args.batchsize,\n",
    "        \"dataroot\": args.dataroot,\n",
    "        \"snapshot\": args.snapshot,\n",
    "        \"resume\": args.resume,\n",
    "        \"architecture\": \"EarlyRNN\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"criterion\": \"EarlyRewardLoss\",\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get doys dict test\n",
      "get doys dict test done\n",
      "get train and validation data...\n",
      "2559635960 2559635960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 178613/178613 [01:11<00:00, 2487.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253658856 2253658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 140645/140645 [00:57<00:00, 2436.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493572704 2493572704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 166391/166391 [01:03<00:00, 2611.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['barley' 'wheat' 'rapeseed' 'corn' 'sunflower' 'orchards' 'nuts'\n",
      " 'permanent meadows' 'temporary meadows']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- LOAD DATASET -----------------------------\n",
    "\n",
    "if args.dataset == \"bavariancrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"bavariancrops\")\n",
    "    nclasses = 7\n",
    "    input_dim = 13\n",
    "    class_weights = None\n",
    "    train_ds = BavarianCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BavarianCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset == \"unitedstates\":\n",
    "    args.dataroot = \"/data/modiscdl/\"\n",
    "    args.sequencelength = 24\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 8\n",
    "    input_dim = 1\n",
    "    train_ds = ModisCDL(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = ModisCDL(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "elif args.dataset == \"breizhcrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"breizhcrops\")\n",
    "    nclasses = 9\n",
    "    input_dim = 13\n",
    "    print(\"get doys dict test\")\n",
    "    doys_dict_test = get_doys_dict_test(dataroot=os.path.join(args.dataroot,args.dataset))\n",
    "    length_sorted_doy_dict_test = create_sorted_doys_dict_test(doys_dict_test)\n",
    "    print(\"get doys dict test done\")\n",
    "    print(\"get train and validation data...\")\n",
    "    train_ds = BreizhCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BreizhCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.ds.classname\n",
    "    print(\"class names:\", class_names)\n",
    "elif args.dataset in [\"ghana\"]:\n",
    "    use_s2_only = False\n",
    "    average_pixel = False\n",
    "    max_n_pixels = 50\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    input_dim = 12 if use_s2_only else 19  # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    args.sequencelength = 365\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength,\n",
    "                                    country=\"ghana\",\n",
    "                                    use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                    max_n_pixels=max_n_pixels)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "\n",
    "    test_ds = SustainbenchCrops(root=dataroot,partition=\"test\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset in [\"southsudan\"]:\n",
    "    use_s2_only = False\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    args.sequencelength = 365\n",
    "    input_dim = 12 if use_s2_only else 19 # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "    test_ds = SustainbenchCrops(root=dataroot, partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    class_names = test_ds.classes\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"dataset {args.dataset} not recognized\")\n",
    "\n",
    "traindataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batchsize)\n",
    "testdataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=args.batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- VISUALIZATION: label distribution -----------------------------\n",
    "# datasets = [train_ds, test_ds]\n",
    "# sets_labels = [\"Train\", \"Validation\"]\n",
    "# fig, ax = plt.subplots(figsize=(15, 7))\n",
    "# fig, ax = plot_label_distribution_datasets(datasets, sets_labels, fig, ax, title='Label distribution', labels_names=class_names)\n",
    "# wandb.log({\"label_distribution\": wandb.Image(fig)})\n",
    "# plt.close(fig)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_lengths_train [25, 50]\n",
      "extra_padding_list [45, 20]\n"
     ]
    }
   ],
   "source": [
    "step_timestamp_padding = 25\n",
    "sequence_lengths_train = [step_timestamp_padding*i for i in range(1, args.sequencelength//step_timestamp_padding+1)]\n",
    "print(\"sequence_lengths_train\", sequence_lengths_train)\n",
    "extra_padding_list = [args.sequencelength - i for i in sequence_lengths_train]\n",
    "print(\"extra_padding_list\", extra_padding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import os\n",
    "#from models.EarlyClassificationModel import EarlyClassificationModel\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "from models.TempCNN import TempCNN\n",
    "\n",
    "\n",
    "class EarlyRNN(nn.Module):\n",
    "    def __init__(self, backbone_model:str=\"LSTM\", input_dim:int=13, hidden_dims:int=64, nclasses:int=7, num_rnn_layers:int=2, dropout:float=0.2, sequencelength:int=70, kernel_size:int=7):\n",
    "        super(EarlyRNN, self).__init__()\n",
    "        # padding layer, for certain models that require padding\n",
    "        self.padding = Padding()\n",
    "\n",
    "        # input transformations\n",
    "        self.intransforms = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim), # normalization over D-dimension. T-dimension is untouched\n",
    "            nn.Linear(input_dim, hidden_dims) # project to hidden_dims length\n",
    "        )\n",
    "\n",
    "        # backbone model \n",
    "        self.initialize_model(backbone_model, input_dim, hidden_dims, nclasses, num_rnn_layers, dropout, sequencelength, kernel_size)\n",
    "\n",
    "        # Heads\n",
    "        self.classification_head = ClassificationHead(hidden_dims, nclasses)\n",
    "        self.stopping_decision_head = DecisionHead(hidden_dims)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.padding(x, **kwargs)\n",
    "        x = self.intransforms(x)\n",
    "        output_tupple = self.backbone(x)\n",
    "        if type(output_tupple) == tuple:\n",
    "            outputs = output_tupple[0]\n",
    "        else:\n",
    "            outputs = output_tupple # shape : (batch_size, sequencelength, hidden_dims)\n",
    "        log_class_probabilities = self.classification_head(outputs)\n",
    "        probabilitiy_stopping = self.stopping_decision_head(outputs)\n",
    "\n",
    "        return log_class_probabilities, probabilitiy_stopping\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x, **kwargs):\n",
    "        logprobabilities, deltas = self.forward(x, **kwargs)\n",
    "\n",
    "        def sample_stop_decision(delta):\n",
    "            dist = torch.stack([1 - delta, delta], dim=1)\n",
    "            return torch.distributions.Categorical(dist).sample().bool()\n",
    "\n",
    "        batchsize, sequencelength, nclasses = logprobabilities.shape\n",
    "\n",
    "        stop = list()\n",
    "        for t in range(sequencelength):\n",
    "            # stop if sampled true and not stopped previously\n",
    "            if t < sequencelength - 1:\n",
    "                stop_now = sample_stop_decision(deltas[:, t]) # stop decision is true with probability delta\n",
    "                stop.append(stop_now)\n",
    "            else:\n",
    "                # make sure to stop last\n",
    "                last_stop = torch.ones(tuple(stop_now.shape)).bool()\n",
    "                if torch.cuda.is_available():\n",
    "                    last_stop = last_stop.cuda()\n",
    "                stop.append(last_stop)\n",
    "\n",
    "        # stack over the time dimension (multiple stops possible)\n",
    "        stopped = torch.stack(stop, dim=1).bool() # has a true if the model decides to stop at time t\n",
    "\n",
    "        # is only true if stopped for the first time\n",
    "        first_stops = (stopped.cumsum(1) == 1) & stopped\n",
    "\n",
    "        # time of stopping\n",
    "        t_stop = first_stops.long().argmax(1) # get the index of the first stop\n",
    "        \n",
    "        # if there is an extra padding, the stopping time is shifted: \n",
    "        if len(kwargs) > 0:\n",
    "            extra_padding = kwargs.get(\"extra_padding\", 0)\n",
    "            t_stop -= extra_padding\n",
    "\n",
    "        # all predictions\n",
    "        predictions = logprobabilities.argmax(-1)\n",
    "\n",
    "        # predictions at time of stopping\n",
    "        predictions_at_t_stop = torch.masked_select(predictions, first_stops)\n",
    "\n",
    "        return logprobabilities, deltas, predictions_at_t_stop, t_stop\n",
    "    \n",
    "    def initialize_model(self, backbone_model, input_dim, hidden_dims, nclasses, num_rnn_layers, dropout, sequencelength, kernel_size):\n",
    "        self.backbone = get_backbone_model(backbone_model, input_dim, hidden_dims, nclasses, num_rnn_layers, dropout, sequencelength, kernel_size)\n",
    "        \n",
    "\n",
    "def get_backbone_model(backbone_model, input_dim, hidden_dims, nclasses, num_rnn_layers, dropout, sequencelength, kernel_size):\n",
    "    model_map = {\n",
    "        \"LSTM\": {\n",
    "            \"class\": nn.LSTM,\n",
    "            \"config\": {\n",
    "                \"input_size\": hidden_dims,\n",
    "                \"hidden_size\": hidden_dims,\n",
    "                \"num_layers\": num_rnn_layers,\n",
    "                \"bias\": False,\n",
    "                \"batch_first\": True,\n",
    "                \"dropout\": dropout,\n",
    "                \"bidirectional\": False\n",
    "            }\n",
    "        },\n",
    "        \"TempCNN\": {\n",
    "            \"class\": TempCNN,\n",
    "            \"config\": {\n",
    "                \"input_dim\": hidden_dims,\n",
    "                \"sequencelength\": sequencelength,\n",
    "                \"kernel_size\": kernel_size,\n",
    "                \"hidden_dims\": hidden_dims, \n",
    "                \"dropout\": dropout\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if backbone_model in model_map:\n",
    "        model_info = model_map[backbone_model]\n",
    "        return model_info[\"class\"](**model_info[\"config\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Backbone model {backbone_model} is not implemented yet.\")\n",
    "\n",
    "\n",
    "class Padding(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(Padding, self).__init__()\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if len(kwargs) > 0:\n",
    "            extra_padding = kwargs.get(\"extra_padding\", 0)\n",
    "            if extra_padding > 0:\n",
    "                # add extra padding to the input, on the left side\n",
    "                x = torch.cat([torch.zeros(x.shape[0], extra_padding, x.shape[2], device=x.device), x], dim=1)\n",
    "                # remove the extra padding from the output, on the right side\n",
    "                x = x[:, :-extra_padding, :]\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "\n",
    "class ClassificationHead(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dims, nclasses):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dims, nclasses, bias=True),\n",
    "            nn.LogSoftmax(dim=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "class DecisionHead(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dims):\n",
    "        super(DecisionHead, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dims, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # initialize bias to predict late in first epochs\n",
    "        torch.nn.init.normal_(self.projection[0].bias, mean=-2e1, std=1e-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x).squeeze(2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = EarlyRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,196 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- SET UP MODEL -----------------------------\n",
    "#model = EarlyRNN(nclasses=nclasses, input_dim=input_dim, hidden_dims=64, sequencelength=args.sequencelength).to(args.device)\n",
    "# nclasses=9\n",
    "# input_dim=13\n",
    "model = EarlyRNN(args.backbonemodel, nclasses=nclasses, input_dim=input_dim, sequencelength=args.sequencelength, kernel_size=7, hidden_dims=args.hidden_dims).to(args.device)\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters.\")\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "# exclude decision head linear bias from weight decay\n",
    "decay, no_decay = list(), list()\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"stopping_decision_head.projection.0.bias\":\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([{'params': no_decay, 'weight_decay': 0, \"lr\": args.learning_rate}, {'params': decay}],\n",
    "                                lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "criterion = EarlyRewardLoss(alpha=args.alpha, epsilon=args.epsilon)\n",
    "\n",
    "if args.resume and os.path.exists(args.snapshot):\n",
    "    model.load_state_dict(torch.load(args.snapshot, map_location=args.device))\n",
    "    optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                        os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                        )\n",
    "    optimizer.load_state_dict(torch.load(optimizer_snapshot, map_location=args.device))\n",
    "    df = pd.read_csv(args.snapshot + \".csv\")\n",
    "    train_stats = df.to_dict(\"records\")\n",
    "    start_epoch = train_stats[-1][\"epoch\"]\n",
    "    print(f\"resuming from {args.snapshot} epoch {start_epoch}\")\n",
    "else:\n",
    "    train_stats = []\n",
    "    start_epoch = 1\n",
    "\n",
    "not_improved = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers_training.py\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, extra_padding_list=[0]):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        for extra_padding in extra_padding_list:\n",
    "            optimizer.zero_grad()\n",
    "            X, y_true = batch\n",
    "            X, y_true = X.to(device), y_true.to(device)\n",
    "            dict_padding = {\"extra_padding\": extra_padding}\n",
    "            log_class_probabilities, probability_stopping = model(X, **dict_padding)\n",
    "\n",
    "            loss = criterion(log_class_probabilities, probability_stopping, y_true)\n",
    "\n",
    "            if not loss.isnan().any():\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    return np.stack(losses).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=args.device, extra_padding_list=extra_padding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, dataloader, criterion, device, extra_padding_list=[0]):\n",
    "    model.eval()\n",
    "\n",
    "    stats = []\n",
    "    losses = []\n",
    "    slengths = []\n",
    "\n",
    "    # sort the padding in descending order\n",
    "    extra_padding_list = sorted(extra_padding_list, reverse=True)\n",
    "\n",
    "    for ids, batch in enumerate(dataloader):\n",
    "        X, y_true = batch\n",
    "        X, y_true = X.to(device), y_true.to(device)\n",
    "\n",
    "        seqlengths = (X[:,:,0] != 0).sum(1)\n",
    "        slengths.append(seqlengths.cpu().detach())\n",
    "\n",
    "        unpredicted_seq_mask = torch.ones(X.shape[0], dtype=bool).to(device) # mask for sequences that are not predicted yet\n",
    "        # by default, we predict the sequence with the smallest padding\n",
    "        extra_padding = extra_padding_list[-1]\n",
    "        dict_padding = {\"extra_padding\": extra_padding}\n",
    "        log_class_probabilities, probability_stopping, predictions_at_t_stop, t_stop = model.predict(X, **dict_padding)\n",
    "        loss, stat = criterion(log_class_probabilities, probability_stopping, y_true, return_stats=True)\n",
    "            \n",
    "        if len(extra_padding_list) > 1:\n",
    "            i=0 # index for the extra_padding_list\n",
    "            while unpredicted_seq_mask.any() and i < len(extra_padding_list)-1:\n",
    "                extra_padding = extra_padding_list[i]\n",
    "                dict_padding = {\"extra_padding\": extra_padding}\n",
    "                log_class_probabilities_temp, probability_stopping_temp, predictions_at_t_stop_temp, t_stop_temp = model.predict(X, **dict_padding)\n",
    "                loss_temp, stat_temp = criterion(log_class_probabilities, probability_stopping, y_true, return_stats=True)\n",
    "                # update the mask if t_stop is different from the length of the sequence (i.e. the sequence is predicted before its end)\n",
    "                unpredicted_seq_mask = unpredicted_seq_mask*(t_stop >= seqlengths-extra_padding)\n",
    "            \n",
    "                # update the metrics data with the mask of predicted sequences\n",
    "                log_class_probabilities = torch.where(~unpredicted_seq_mask.unsqueeze(1).unsqueeze(-1), log_class_probabilities_temp, log_class_probabilities)\n",
    "                probability_stopping = torch.where(~unpredicted_seq_mask.unsqueeze(1), probability_stopping_temp, probability_stopping)\n",
    "                predictions_at_t_stop = torch.where(~unpredicted_seq_mask, predictions_at_t_stop_temp, predictions_at_t_stop)\n",
    "                t_stop = torch.where(~unpredicted_seq_mask, t_stop_temp, t_stop)\n",
    "                loss = torch.where(~unpredicted_seq_mask, loss_temp, loss)\n",
    "                stat = {k: np.where(~np.expand_dims(unpredicted_seq_mask.cpu().numpy(), axis=1), v, stat[k]) for k, v in stat_temp.items()}\n",
    "                i+=1\n",
    "            # mean: so not exact measure, as it takes into account predictions that are not kept at the end. Only indicative. \n",
    "            stat[\"classification_loss\"] = stat[\"classification_loss\"].mean() \n",
    "            stat[\"earliness_reward\"] = stat[\"earliness_reward\"].mean()\n",
    "            loss = loss.mean()\n",
    "        stat[\"loss\"] = loss.cpu().detach().numpy()\n",
    "        stat[\"probability_stopping\"] = probability_stopping.cpu().detach().numpy()\n",
    "        stat[\"class_probabilities\"] = log_class_probabilities.exp().cpu().detach().numpy()\n",
    "        stat[\"predictions_at_t_stop\"] = predictions_at_t_stop.unsqueeze(-1).cpu().detach().numpy()\n",
    "        stat[\"t_stop\"] = t_stop.unsqueeze(-1).cpu().detach().numpy()\n",
    "        stat[\"targets\"] = y_true.cpu().detach().numpy()\n",
    "        stat[\"ids\"] = ids\n",
    "\n",
    "        stats.append(stat)\n",
    "        losses.append(loss.mean().cpu().detach().numpy())\n",
    "\n",
    "    # list of dicts to dict of lists\n",
    "    stats = {k: np.vstack([dic[k] for dic in stats]) for k in stats[0]}\n",
    "    stats[\"seqlengths\"] = torch.cat(slengths).numpy()\n",
    "    stats[\"classification_earliness\"] = np.mean(stats[\"t_stop\"].flatten()/stats[\"seqlengths\"])\n",
    "\n",
    "    return np.stack(losses).mean(), stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss, stats = test_epoch(model, testdataloader, criterion, args.device, extra_padding_list=extra_padding_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch_old(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    stats = []\n",
    "    losses = []\n",
    "    slengths = []\n",
    "    for ids, batch in enumerate(dataloader):\n",
    "        X, y_true = batch\n",
    "        X, y_true = X.to(device), y_true.to(device)\n",
    "\n",
    "        seqlengths = (X[:,:,0] != 0).sum(1)\n",
    "        slengths.append(seqlengths.cpu().detach())\n",
    "\n",
    "        log_class_probabilities, probability_stopping, predictions_at_t_stop, t_stop = model.predict(X)\n",
    "        loss, stat = criterion(log_class_probabilities, probability_stopping, y_true, return_stats=True)\n",
    "\n",
    "        stat[\"loss\"] = loss.cpu().detach().numpy()\n",
    "        stat[\"probability_stopping\"] = probability_stopping.cpu().detach().numpy()\n",
    "        stat[\"class_probabilities\"] = log_class_probabilities.exp().cpu().detach().numpy()\n",
    "        stat[\"predictions_at_t_stop\"] = predictions_at_t_stop.unsqueeze(-1).cpu().detach().numpy()\n",
    "        stat[\"t_stop\"] = t_stop.unsqueeze(-1).cpu().detach().numpy()\n",
    "        stat[\"targets\"] = y_true.cpu().detach().numpy()\n",
    "        stat[\"ids\"] = ids\n",
    "\n",
    "        stats.append(stat)\n",
    "\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    # list of dicts to dict of lists\n",
    "    stats = {k: np.vstack([dic[k] for dic in stats]) for k in stats[0]}\n",
    "    stats[\"seqlengths\"] = torch.cat(slengths).numpy()\n",
    "    stats[\"classification_earliness\"] = np.mean(stats[\"t_stop\"].flatten()/stats[\"seqlengths\"])\n",
    "\n",
    "    return np.stack(losses).mean(), stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time test_epoch 63.539100646972656\n",
      "time test_epoch_old 61.57906174659729\n",
      "time test_epoch_old2 61.03752112388611\n"
     ]
    }
   ],
   "source": [
    "# measure the time of the test_epoch function\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "testloss, stats = test_epoch(model, testdataloader, criterion, args.device, extra_padding_list=[0])\n",
    "end = time.time()\n",
    "print(\"time test_epoch\", end-start)\n",
    "start = time.time()\n",
    "testloss_old, stats_old = test_epoch_old(model, testdataloader, criterion, args.device)\n",
    "end = time.time()\n",
    "print(\"time test_epoch_old\", end-start)\n",
    "start = time.time()\n",
    "testloss2, stats_old2 = test_epoch_old(model, testdataloader, criterion, args.device)\n",
    "end = time.time()\n",
    "print(\"time test_epoch_old2\", end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testloss 4.576465\n",
      "testloss_old 4.574134\n",
      "testloss2 4.577822\n",
      "difference: 0.0023312569\n",
      "difference2: 0.0036883354\n"
     ]
    }
   ],
   "source": [
    "#both should give the same result: \n",
    "print(\"testloss\", testloss)\n",
    "print(\"testloss_old\", testloss_old)\n",
    "print(\"testloss2\", testloss2)\n",
    "print(\"difference:\", np.abs(testloss-testloss_old))\n",
    "print(\"difference2:\", np.abs(testloss_old-testloss2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allclose: True\n",
      "allclose2: True\n"
     ]
    }
   ],
   "source": [
    "key = \"targets\"\n",
    "# check that the results are the same for key \n",
    "print(\"allclose:\", np.allclose(stats[key], stats_old[key]))\n",
    "print(\"allclose2:\", np.allclose(stats[key], stats_old2[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training for epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: trainloss 5.31, testloss 6.09, accuracy 0.75, earliness 0.75. classification loss 15.43, earliness reward 3.26, harmonic mean 0.74. :  10%|█         | 1/10 [03:53<34:57, 233.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m----> 5\u001b[0m         trainloss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraindataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_padding_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_padding_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished training for epoch \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m      7\u001b[0m         testloss, stats \u001b[38;5;241m=\u001b[39m test_epoch(model, testdataloader, criterion, args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device, extra_padding_list)\u001b[0m\n\u001b[0;32m     15\u001b[0m dict_padding \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_padding\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_padding}\n\u001b[0;32m     16\u001b[0m log_class_probabilities, probability_stopping \u001b[38;5;241m=\u001b[39m model(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdict_padding)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_class_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobability_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loss\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\loss.py:16\u001b[0m, in \u001b[0;36mEarlyRewardLoss.forward\u001b[1;34m(self, log_class_probabilities, probability_stopping, y_true, return_stats)\u001b[0m\n\u001b[0;32m     13\u001b[0m N, T, C \u001b[38;5;241m=\u001b[39m log_class_probabilities\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# equation 3\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m Pt \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_probability_making_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobability_stopping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# equation 8 additive smoothing\u001b[39;00m\n\u001b[0;32m     19\u001b[0m Pt \u001b[38;5;241m=\u001b[39m Pt \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m/\u001b[39m T\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\loss.py:62\u001b[0m, in \u001b[0;36mcalculate_probability_making_decision\u001b[1;34m(deltas)\u001b[0m\n\u001b[0;32m     60\u001b[0m     pt \u001b[38;5;241m=\u001b[39m deltas[:, t] \u001b[38;5;241m*\u001b[39m budget[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     61\u001b[0m     budget\u001b[38;5;241m.\u001b[39mappend(budget[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m pt)\n\u001b[1;32m---> 62\u001b[0m     pts\u001b[38;5;241m.\u001b[39mappend(pt)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# last time\u001b[39;00m\n\u001b[0;32m     65\u001b[0m pt \u001b[38;5;241m=\u001b[39m budget[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------- TRAINING -----------------------------\n",
    "print(\"starting training...\")\n",
    "with tqdm(range(start_epoch, args.epochs + 1)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=args.device, extra_padding_list=extra_padding_list)\n",
    "        print(\"finished training for epoch \", epoch)\n",
    "        testloss, stats = test_epoch(model, testdataloader, criterion, args.device, thresh_stop=0.8, extra_padding_list=extra_padding_list)\n",
    "\n",
    "        # statistic logging and visualization...\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0], average=\"macro\",\n",
    "            zero_division=0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0])\n",
    "        kappa = sklearn.metrics.cohen_kappa_score(\n",
    "            stats[\"predictions_at_t_stop\"][:, 0], stats[\"targets\"][:, 0])\n",
    "\n",
    "        classification_loss = stats[\"classification_loss\"].mean()\n",
    "        earliness_reward = stats[\"earliness_reward\"].mean()\n",
    "        earliness = 1 - (stats[\"t_stop\"].mean() / (args.sequencelength - 1))\n",
    "        harmonic_mean = harmonic_mean_score(accuracy, stats[\"classification_earliness\"])\n",
    "\n",
    "        # ----------------------------- LOGGING -----------------------------\n",
    "        train_stats.append(\n",
    "            dict(\n",
    "                epoch=epoch,\n",
    "                trainloss=trainloss,\n",
    "                testloss=testloss,\n",
    "                accuracy=accuracy,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                fscore=fscore,\n",
    "                kappa=kappa,\n",
    "                elects_earliness=earliness,\n",
    "                classification_loss=classification_loss,\n",
    "                earliness_reward=earliness_reward,\n",
    "                classification_earliness=stats[\"classification_earliness\"],\n",
    "                harmonic_mean=harmonic_mean,\n",
    "            )\n",
    "        )\n",
    "        fig_boxplot, ax_boxplot = plt.subplots(figsize=(15, 7))\n",
    "        doys_dict = get_approximated_doys_dict(stats[\"seqlengths\"], length_sorted_doy_dict_test)\n",
    "        doys_stop = get_doy_stop(stats, doys_dict)\n",
    "        fig_boxplot, _ = boxplot_stopping_times(doys_stop, stats, fig_boxplot, ax_boxplot, class_names)\n",
    "        wandb.log({\n",
    "                \"loss\": {\"trainloss\": trainloss, \"testloss\": testloss},\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"fscore\": fscore,\n",
    "                \"kappa\": kappa,\n",
    "                \"elects_earliness\": earliness,\n",
    "                \"classification_loss\": classification_loss,\n",
    "                \"earliness_reward\": earliness_reward,\n",
    "                \"classification_earliness\": stats[\"classification_earliness\"],\n",
    "                \"harmonic_mean\": harmonic_mean,\n",
    "                \"boxplot\": wandb.Image(fig_boxplot),\n",
    "                \"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=stats[\"targets\"][:,0], preds=stats[\"predictions_at_t_stop\"][:,0],\n",
    "                        class_names=class_names, title=\"Confusion Matrix\")\n",
    "            })\n",
    "        plt.close(fig_boxplot)\n",
    "\n",
    "        df = pd.DataFrame(train_stats).set_index(\"epoch\")\n",
    "\n",
    "        savemsg = \"\"\n",
    "        if len(df) > 2:\n",
    "            if testloss < df.testloss[:-1].values.min():\n",
    "                savemsg = f\"saving model to {args.snapshot}\"\n",
    "                os.makedirs(os.path.dirname(args.snapshot), exist_ok=True)\n",
    "                torch.save(model.state_dict(), args.snapshot)\n",
    "\n",
    "                optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                                    os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                                    )\n",
    "                torch.save(optimizer.state_dict(), optimizer_snapshot)\n",
    "                wandb.log_artifact(args.snapshot, type=\"model\")  \n",
    "\n",
    "                df.to_csv(args.snapshot + \".csv\")\n",
    "                not_improved = 0 # reset early stopping counter\n",
    "            else:\n",
    "                not_improved += 1 # increment early stopping counter\n",
    "                if args.patience is not None:\n",
    "                    savemsg = f\"early stopping in {args.patience - not_improved} epochs.\"\n",
    "                else:\n",
    "                    savemsg = \"\"\n",
    "\n",
    "        pbar.set_description(f\"epoch {epoch}: trainloss {trainloss:.2f}, testloss {testloss:.2f}, \"\n",
    "                     f\"accuracy {accuracy:.2f}, earliness {earliness:.2f}. \"\n",
    "                     f\"classification loss {classification_loss:.2f}, earliness reward {earliness_reward:.2f}, harmonic mean {harmonic_mean:.2f}. {savemsg}\")\n",
    "        \n",
    "            \n",
    "        if args.patience is not None:\n",
    "            if not_improved > args.patience:\n",
    "                print(f\"stopping training. testloss {testloss:.2f} did not improve in {args.patience} epochs.\")\n",
    "                break\n",
    "    \n",
    "# ----------------------------- SAVE FINAL MODEL -----------------------------\n",
    "wandb.log_artifact(args.snapshot, type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbdfdaa38b84d6988fbaaabee716ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elects_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
