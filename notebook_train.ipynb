{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Notebook - for developing code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "os.environ['MPLCONFIGDIR'] = '/myhome'\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "from data import BavarianCrops, BreizhCrops, SustainbenchCrops, ModisCDL\n",
    "from torch.utils.data import DataLoader\n",
    "from earlyrnn import EarlyRNN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from loss import EarlyRewardLoss\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from utils.plots import plot_label_distribution_datasets, boxplot_stopping_times\n",
    "from utils.doy import get_doys_dict_test, get_doy_stop, create_sorted_doys_dict_test, get_approximated_doys_dict\n",
    "from utils.helpers_training import parse_args, train_epoch\n",
    "from utils.helpers_testing import test_epoch\n",
    "from utils.metrics import harmonic_mean_score\n",
    "import matplotlib.pyplot as plt\n",
    "from models.model_helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args(args=None):\n",
    "    def int_list(value):\n",
    "        # This function will split the string by spaces and convert each to an integer\n",
    "        return [int(i) for i in value.split()]\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Run ELECTS Early Classification training on the BavarianCrops dataset.')\n",
    "    parser.add_argument('--backbonemodel', type=str, default=\"LSTM\", choices=[\"LSTM\", \"TempCNN\", \"Transformer\"], help=\"backbone model\")\n",
    "    parser.add_argument('--dataset', type=str, default=\"bavariancrops\", choices=[\"bavariancrops\",\"breizhcrops\", \"ghana\", \"southsudan\",\"unitedstates\"], help=\"dataset\")\n",
    "    parser.add_argument('--alpha', type=float, default=0.5, help=\"trade-off parameter of earliness and accuracy (eq 6): \"\n",
    "                                                                 \"1=full weight on accuracy; 0=full weight on earliness\")\n",
    "    parser.add_argument('--epsilon', type=float, default=10, help=\"additive smoothing parameter that helps the \"\n",
    "                                                                  \"model recover from too early classifications (eq 7)\")\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-3, help=\"Optimizer learning rate\")\n",
    "    parser.add_argument('--weight-decay', type=float, default=0, help=\"weight_decay\")\n",
    "    parser.add_argument('--patience', type=int, default=30, help=\"Early stopping patience\")\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                        choices=[\"cuda\", \"cpu\"], help=\"'cuda' (GPU) or 'cpu' device to run the code. \"\n",
    "                                                     \"defaults to 'cuda' if GPU is available, otherwise 'cpu'\")\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"number of training epochs\")\n",
    "    parser.add_argument('--sequencelength', type=int, default=70, help=\"sequencelength of the time series. If samples are shorter, \"\n",
    "                                                                \"they are zero-padded until this length; \"\n",
    "                                                                \"if samples are longer, they will be undersampled\")\n",
    "    parser.add_argument('--extra-padding-list', type=int_list, default=[[0]], nargs='+', help=\"extra padding for the TempCNN model\")\n",
    "    parser.add_argument('--hidden-dims', type=int, default=64, help=\"number of hidden dimensions in the backbone model\")\n",
    "    parser.add_argument('--batchsize', type=int, default=256, help=\"number of samples per batch\")\n",
    "    parser.add_argument('--dataroot', type=str, default=os.path.join(os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\")),\"elects_data\"), help=\"directory to download the \"\n",
    "                                                                                 \"BavarianCrops dataset (400MB).\"\n",
    "                                                                                 \"Defaults to home directory.\")\n",
    "    parser.add_argument('--snapshot', type=str, default=\"snapshots/model.pth\",\n",
    "                        help=\"pytorch state dict snapshot file\")\n",
    "    parser.add_argument('--left_padding', type=bool, default=False, help=\"left padding for the TempCNN model\")\n",
    "    parser.add_argument('--resume', action='store_true')\n",
    "\n",
    "    if args is not None:\n",
    "        args = parser.parse_args(args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if args.patience < 0:\n",
    "        args.patience = None\n",
    "    args.extra_padding_list = [item for sublist in args.extra_padding_list for item in sublist]\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available:  cuda\n",
      "Namespace(backbonemodel='TempCNN', dataset='breizhcrops', alpha=0.5, epsilon=10, learning_rate=0.001, weight_decay=0, patience=30, device='cuda', epochs=10, sequencelength=70, extra_padding_list=[35, 0], hidden_dims=16, batchsize=256, dataroot='C:\\\\Users\\\\anyam\\\\elects_data', snapshot='snapshots/model.pth', left_padding=False, resume=False)\n"
     ]
    }
   ],
   "source": [
    "# Example of how to call parse_args with custom arguments in a notebook\n",
    "custom_args = \"--backbonemodel TempCNN --dataset breizhcrops --epochs 10 --hidden-dims 16 --sequencelength 70 --extra-padding-list 35 0\".split()\n",
    "args = parse_args(custom_args)\n",
    "print(\"cuda is available: \", args.device)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: aurenore. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\wandb\\run-20240424_142114-c71l6rap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aurenore/ELECTS/runs/c71l6rap/workspace' target=\"_blank\">vibrant-planet-90</a></strong> to <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">https://wandb.ai/aurenore/ELECTS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aurenore/ELECTS/runs/c71l6rap/workspace' target=\"_blank\">https://wandb.ai/aurenore/ELECTS/runs/c71l6rap/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aurenore/ELECTS/runs/c71l6rap?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x16b9a505a50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/mydata/studentanya/anya/wandb/\"\n",
    "tag_padding = \"left padding\" if args.left_padding else \"right padding\"\n",
    "wandb.init(\n",
    "        dir=None,\n",
    "        project=\"ELECTS\",\n",
    "        notes=\"first experimentations with ELECTS\",\n",
    "        tags=[\"ELECTS\", args.dataset, \"with_doys_boxplot\", args.backbonemodel, tag_padding, \"in notebook_train\"],\n",
    "        config={\n",
    "        \"backbonemodel\": args.backbonemodel,\n",
    "        \"dataset\": args.dataset,\n",
    "        \"alpha\": args.alpha,\n",
    "        \"epsilon\": args.epsilon,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "        \"patience\": args.patience,\n",
    "        \"device\": args.device,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"sequencelength\": args.sequencelength,\n",
    "        \"extra_padding_list\": args.extra_padding_list,\n",
    "        \"hidden_dims\": args.hidden_dims,\n",
    "        \"batchsize\": args.batchsize,\n",
    "        \"dataroot\": args.dataroot,\n",
    "        \"snapshot\": args.snapshot,\n",
    "        \"resume\": args.resume,\n",
    "        \"left_padding\": args.left_padding,\n",
    "        \"architecture\": \"EarlyRNN\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"criterion\": \"EarlyRewardLoss\",\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get doys dict test\n",
      "get doys dict test done\n",
      "get train and validation data...\n",
      "2559635960 2559635960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 178613/178613 [01:17<00:00, 2316.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253658856 2253658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 140645/140645 [00:59<00:00, 2379.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493572704 2493572704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 166391/166391 [01:11<00:00, 2339.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['barley' 'wheat' 'rapeseed' 'corn' 'sunflower' 'orchards' 'nuts'\n",
      " 'permanent meadows' 'temporary meadows']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- LOAD DATASET -----------------------------\n",
    "\n",
    "if args.dataset == \"bavariancrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"bavariancrops\")\n",
    "    nclasses = 7\n",
    "    input_dim = 13\n",
    "    class_weights = None\n",
    "    train_ds = BavarianCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BavarianCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset == \"unitedstates\":\n",
    "    args.dataroot = \"/data/modiscdl/\"\n",
    "    args.sequencelength = 24\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 8\n",
    "    input_dim = 1\n",
    "    train_ds = ModisCDL(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = ModisCDL(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "elif args.dataset == \"breizhcrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"breizhcrops\")\n",
    "    nclasses = 9\n",
    "    input_dim = 13\n",
    "    print(\"get doys dict test\")\n",
    "    doys_dict_test = get_doys_dict_test(dataroot=os.path.join(args.dataroot,args.dataset))\n",
    "    length_sorted_doy_dict_test = create_sorted_doys_dict_test(doys_dict_test)\n",
    "    print(\"get doys dict test done\")\n",
    "    print(\"get train and validation data...\")\n",
    "    train_ds = BreizhCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BreizhCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.ds.classname\n",
    "    print(\"class names:\", class_names)\n",
    "elif args.dataset in [\"ghana\"]:\n",
    "    use_s2_only = False\n",
    "    average_pixel = False\n",
    "    max_n_pixels = 50\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    input_dim = 12 if use_s2_only else 19  # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    args.sequencelength = 365\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength,\n",
    "                                    country=\"ghana\",\n",
    "                                    use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                    max_n_pixels=max_n_pixels)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "\n",
    "    test_ds = SustainbenchCrops(root=dataroot,partition=\"test\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset in [\"southsudan\"]:\n",
    "    use_s2_only = False\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    args.sequencelength = 365\n",
    "    input_dim = 12 if use_s2_only else 19 # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "    test_ds = SustainbenchCrops(root=dataroot, partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    class_names = test_ds.classes\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"dataset {args.dataset} not recognized\")\n",
    "\n",
    "traindataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batchsize)\n",
    "testdataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=args.batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- VISUALIZATION: label distribution -----------------------------\n",
    "# datasets = [train_ds, test_ds]\n",
    "# sets_labels = [\"Train\", \"Validation\"]\n",
    "# fig, ax = plt.subplots(figsize=(15, 7))\n",
    "# fig, ax = plot_label_distribution_datasets(datasets, sets_labels, fig, ax, title='Label distribution', labels_names=class_names)\n",
    "# wandb.log({\"label_distribution\": wandb.Image(fig)})\n",
    "# plt.close(fig)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_lengths_train [25, 50]\n",
      "extra_padding_list [45, 20]\n"
     ]
    }
   ],
   "source": [
    "step_timestamp_padding = 25\n",
    "sequence_lengths_train = [step_timestamp_padding*i for i in range(1, args.sequencelength//step_timestamp_padding+1)]\n",
    "print(\"sequence_lengths_train\", sequence_lengths_train)\n",
    "extra_padding_list = [args.sequencelength - i for i in sequence_lengths_train]\n",
    "print(\"extra_padding_list\", extra_padding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 8,196 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- SET UP MODEL -----------------------------\n",
    "#model = EarlyRNN(nclasses=nclasses, input_dim=input_dim, hidden_dims=64, sequencelength=args.sequencelength).to(args.device)\n",
    "# nclasses=9\n",
    "# input_dim=13\n",
    "model = EarlyRNN(args.backbonemodel, nclasses=nclasses, input_dim=input_dim, sequencelength=args.sequencelength, kernel_size=7, hidden_dims=args.hidden_dims, left_padding=args.left_padding).to(args.device)\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters.\")\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "# exclude decision head linear bias from weight decay\n",
    "decay, no_decay = list(), list()\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"stopping_decision_head.projection.0.bias\":\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([{'params': no_decay, 'weight_decay': 0, \"lr\": args.learning_rate}, {'params': decay}],\n",
    "                                lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "criterion = EarlyRewardLoss(alpha=args.alpha, epsilon=args.epsilon)\n",
    "\n",
    "if args.resume and os.path.exists(args.snapshot):\n",
    "    model.load_state_dict(torch.load(args.snapshot, map_location=args.device))\n",
    "    optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                        os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                        )\n",
    "    optimizer.load_state_dict(torch.load(optimizer_snapshot, map_location=args.device))\n",
    "    df = pd.read_csv(args.snapshot + \".csv\")\n",
    "    train_stats = df.to_dict(\"records\")\n",
    "    start_epoch = train_stats[-1][\"epoch\"]\n",
    "    print(f\"resuming from {args.snapshot} epoch {start_epoch}\")\n",
    "else:\n",
    "    train_stats = []\n",
    "    start_epoch = 1\n",
    "\n",
    "not_improved = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training for epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [04:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m trainloss \u001b[38;5;241m=\u001b[39m train_epoch(model, traindataloader, optimizer, criterion, device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice, extra_padding_list\u001b[38;5;241m=\u001b[39mextra_padding_list)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished training for epoch \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[1;32m----> 7\u001b[0m testloss, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtest_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_padding_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_padding_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# statistic logging and visualization...\u001b[39;00m\n\u001b[0;32m     10\u001b[0m precision, recall, fscore, support \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mprecision_recall_fscore_support(\n\u001b[0;32m     11\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions_at_t_stop\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m0\u001b[39m], y_true\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m0\u001b[39m], average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\utils\\helpers_testing.py:46\u001b[0m, in \u001b[0;36mtest_epoch\u001b[1;34m(model, dataloader, criterion, device, extra_padding_list, return_id)\u001b[0m\n\u001b[0;32m     44\u001b[0m extra_padding \u001b[38;5;241m=\u001b[39m extra_padding_list[i]\n\u001b[0;32m     45\u001b[0m dict_padding \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_padding\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_padding}\n\u001b[1;32m---> 46\u001b[0m log_class_probabilities_temp, probability_stopping_temp, predictions_at_t_stop_temp, t_stop_temp \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdict_padding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# update the mask if t_stop is different from the length of the padded sequence (i.e. the sequence is predicted before its end)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m unpredicted_seq_mask \u001b[38;5;241m=\u001b[39m unpredicted_seq_mask\u001b[38;5;241m*\u001b[39m(t_stop \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m seqlengths\u001b[38;5;241m-\u001b[39mextra_padding)\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\earlyrnn.py:58\u001b[0m, in \u001b[0;36mEarlyRNN.predict\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequencelength):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# stop if sampled true and not stopped previously\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m<\u001b[39m sequencelength \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 58\u001b[0m         stop_now \u001b[38;5;241m=\u001b[39m \u001b[43msample_stop_decision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# stop decision is true with probability delta\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         stop\u001b[38;5;241m.\u001b[39mappend(stop_now)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# make sure to stop last\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\earlyrnn.py:50\u001b[0m, in \u001b[0;36mEarlyRNN.predict.<locals>.sample_stop_decision\u001b[1;34m(delta)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_stop_decision\u001b[39m(delta):\n\u001b[0;32m     49\u001b[0m     dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m delta, delta], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbool()\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\distributions\\categorical.py:132\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    130\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[0;32m    131\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[1;32m--> 132\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------- TRAINING -----------------------------\n",
    "print(\"starting training...\")\n",
    "with tqdm(range(start_epoch, args.epochs + 1)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=args.device, extra_padding_list=extra_padding_list)\n",
    "        print(\"finished training for epoch \", epoch)\n",
    "        testloss, stats = test_epoch(model, testdataloader, criterion, args.device, extra_padding_list=extra_padding_list, return_id=test_ds.return_id)\n",
    "\n",
    "        # statistic logging and visualization...\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0], average=\"macro\",\n",
    "            zero_division=0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0])\n",
    "        kappa = sklearn.metrics.cohen_kappa_score(\n",
    "            stats[\"predictions_at_t_stop\"][:, 0], stats[\"targets\"][:, 0])\n",
    "\n",
    "        classification_loss = stats[\"classification_loss\"].mean()\n",
    "        earliness_reward = stats[\"earliness_reward\"].mean()\n",
    "        earliness = 1 - (stats[\"t_stop\"].mean() / (args.sequencelength - 1))\n",
    "        harmonic_mean = harmonic_mean_score(accuracy, stats[\"classification_earliness\"])\n",
    "\n",
    "        # ----------------------------- LOGGING -----------------------------\n",
    "        train_stats.append(\n",
    "            dict(\n",
    "                epoch=epoch,\n",
    "                trainloss=trainloss,\n",
    "                testloss=testloss,\n",
    "                accuracy=accuracy,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                fscore=fscore,\n",
    "                kappa=kappa,\n",
    "                elects_earliness=earliness,\n",
    "                classification_loss=classification_loss,\n",
    "                earliness_reward=earliness_reward,\n",
    "                classification_earliness=stats[\"classification_earliness\"],\n",
    "                harmonic_mean=harmonic_mean,\n",
    "            )\n",
    "        )\n",
    "        fig_boxplot, ax_boxplot = plt.subplots(figsize=(15, 7))\n",
    "        doys_dict = get_approximated_doys_dict(stats[\"seqlengths\"], length_sorted_doy_dict_test)\n",
    "        doys_stop = get_doy_stop(stats, doys_dict)\n",
    "        fig_boxplot, _ = boxplot_stopping_times(doys_stop, stats, fig_boxplot, ax_boxplot, class_names)\n",
    "        wandb.log({\n",
    "                \"loss\": {\"trainloss\": trainloss, \"testloss\": testloss},\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"fscore\": fscore,\n",
    "                \"kappa\": kappa,\n",
    "                \"elects_earliness\": earliness,\n",
    "                \"classification_loss\": classification_loss,\n",
    "                \"earliness_reward\": earliness_reward,\n",
    "                \"classification_earliness\": stats[\"classification_earliness\"],\n",
    "                \"harmonic_mean\": harmonic_mean,\n",
    "                \"boxplot\": wandb.Image(fig_boxplot),\n",
    "                \"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=stats[\"targets\"][:,0], preds=stats[\"predictions_at_t_stop\"][:,0],\n",
    "                        class_names=class_names, title=\"Confusion Matrix\")\n",
    "            })\n",
    "        plt.close(fig_boxplot)\n",
    "\n",
    "        df = pd.DataFrame(train_stats).set_index(\"epoch\")\n",
    "\n",
    "        savemsg = \"\"\n",
    "        if len(df) > 2:\n",
    "            if testloss < df.testloss[:-1].values.min():\n",
    "                savemsg = f\"saving model to {args.snapshot}\"\n",
    "                os.makedirs(os.path.dirname(args.snapshot), exist_ok=True)\n",
    "                torch.save(model.state_dict(), args.snapshot)\n",
    "\n",
    "                optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                                    os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                                    )\n",
    "                torch.save(optimizer.state_dict(), optimizer_snapshot)\n",
    "                wandb.log_artifact(args.snapshot, type=\"model\")  \n",
    "\n",
    "                df.to_csv(args.snapshot + \".csv\")\n",
    "                not_improved = 0 # reset early stopping counter\n",
    "            else:\n",
    "                not_improved += 1 # increment early stopping counter\n",
    "                if args.patience is not None:\n",
    "                    savemsg = f\"early stopping in {args.patience - not_improved} epochs.\"\n",
    "                else:\n",
    "                    savemsg = \"\"\n",
    "\n",
    "        pbar.set_description(f\"epoch {epoch}: trainloss {trainloss:.2f}, testloss {testloss:.2f}, \"\n",
    "                     f\"accuracy {accuracy:.2f}, earliness {earliness:.2f}. \"\n",
    "                     f\"classification loss {classification_loss:.2f}, earliness reward {earliness_reward:.2f}, harmonic mean {harmonic_mean:.2f}. {savemsg}\")\n",
    "        \n",
    "            \n",
    "        if args.patience is not None:\n",
    "            if not_improved > args.patience:\n",
    "                print(f\"stopping training. testloss {testloss:.2f} did not improve in {args.patience} epochs.\")\n",
    "                break\n",
    "    \n",
    "# ----------------------------- SAVE FINAL MODEL -----------------------------\n",
    "wandb.log_artifact(args.snapshot, type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7185e4f954545b5b1ac7372b935271b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-planet-90</strong> at: <a href='https://wandb.ai/aurenore/ELECTS/runs/c71l6rap/workspace' target=\"_blank\">https://wandb.ai/aurenore/ELECTS/runs/c71l6rap/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240424_142114-c71l6rap\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elects_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
