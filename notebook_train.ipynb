{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Notebook - for developing code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "os.environ['MPLCONFIGDIR'] = '/myhome'\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "from data import BavarianCrops, BreizhCrops, SustainbenchCrops, ModisCDL\n",
    "from torch.utils.data import DataLoader\n",
    "from earlyrnn import EarlyRNN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from loss import EarlyRewardLoss\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from utils.plots import plot_label_distribution_datasets, boxplot_stopping_times\n",
    "from utils.doy import get_doys_dict_test, get_doy_stop, create_sorted_doys_dict_test, get_approximated_doys_dict\n",
    "from utils.helpers_training import parse_args, train_epoch, test_epoch\n",
    "from utils.metrics import harmonic_mean_score\n",
    "import matplotlib.pyplot as plt\n",
    "from models.model_helpers import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Run ELECTS Early Classification training on the BavarianCrops dataset.')\n",
    "    parser.add_argument('--dataset', type=str, default=\"bavariancrops\", choices=[\"bavariancrops\",\"breizhcrops\", \"ghana\", \"southsudan\",\"unitedstates\"], help=\"dataset\")\n",
    "    parser.add_argument('--alpha', type=float, default=0.5, help=\"trade-off parameter of earliness and accuracy (eq 6): \"\n",
    "                                                                 \"1=full weight on accuracy; 0=full weight on earliness\")\n",
    "    parser.add_argument('--epsilon', type=float, default=10, help=\"additive smoothing parameter that helps the \"\n",
    "                                                                  \"model recover from too early classifications (eq 7)\")\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-3, help=\"Optimizer learning rate\")\n",
    "    parser.add_argument('--weight-decay', type=float, default=0, help=\"weight_decay\")\n",
    "    parser.add_argument('--patience', type=int, default=30, help=\"Early stopping patience\")\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                        choices=[\"cuda\", \"cpu\"], help=\"'cuda' (GPU) or 'cpu' device to run the code. \"\n",
    "                                                     \"defaults to 'cuda' if GPU is available, otherwise 'cpu'\")\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"number of training epochs\")\n",
    "    parser.add_argument('--sequencelength', type=int, default=70, help=\"sequencelength of the time series. If samples are shorter, \"\n",
    "                                                                \"they are zero-padded until this length; \"\n",
    "                                                                \"if samples are longer, they will be undersampled\")\n",
    "    parser.add_argument('--hidden-dims', type=int, default=64, help=\"number of hidden dimensions in the backbone model\")\n",
    "    parser.add_argument('--batchsize', type=int, default=256, help=\"number of samples per batch\")\n",
    "    parser.add_argument('--dataroot', type=str, default=os.path.join(os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\")),\"elects_data\"), help=\"directory to download the \"\n",
    "                                                                                 \"BavarianCrops dataset (400MB).\"\n",
    "                                                                                 \"Defaults to home directory.\")\n",
    "    parser.add_argument('--snapshot', type=str, default=\"snapshots/model.pth\",\n",
    "                        help=\"pytorch state dict snapshot file\")\n",
    "    parser.add_argument('--resume', action='store_true')\n",
    "\n",
    "    if args is not None:\n",
    "        args = parser.parse_args(args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if args.patience < 0:\n",
    "        args.patience = None\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available:  cuda\n",
      "Namespace(dataset='breizhcrops', alpha=0.5, epsilon=10, learning_rate=0.001, weight_decay=0, patience=30, device='cuda', epochs=10, sequencelength=70, hidden_dims=64, batchsize=256, dataroot='C:\\\\Users\\\\anyam\\\\elects_data', snapshot='snapshots/model.pth', resume=False)\n"
     ]
    }
   ],
   "source": [
    "# Example of how to call parse_args with custom arguments in a notebook\n",
    "custom_args = \"--dataset breizhcrops --epochs 10\".split()\n",
    "args = parse_args(custom_args)\n",
    "print(\"cuda is available: \", args.device)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: aurenore. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\wandb\\run-20240328_135943-hnil6v45</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aurenore/ELECTS/runs/hnil6v45/workspace' target=\"_blank\">deft-sun-79</a></strong> to <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">https://wandb.ai/aurenore/ELECTS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aurenore/ELECTS/runs/hnil6v45/workspace' target=\"_blank\">https://wandb.ai/aurenore/ELECTS/runs/hnil6v45/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aurenore/ELECTS/runs/hnil6v45?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ddbb4d7f90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/mydata/studentanya/anya/wandb/\"\n",
    "wandb.init(\n",
    "        dir=None,\n",
    "        project=\"ELECTS\",\n",
    "        notes=\"first experimentations with ELECTS\",\n",
    "        tags=[\"ELECTS\", args.dataset, \"with_doys_boxplot\"],\n",
    "        config={\n",
    "        \"dataset\": args.dataset,\n",
    "        \"alpha\": args.alpha,\n",
    "        \"epsilon\": args.epsilon,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "        \"patience\": args.patience,\n",
    "        \"device\": args.device,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"sequencelength\": args.sequencelength,\n",
    "        \"hidden_dims\": args.hidden_dims,\n",
    "        \"batchsize\": args.batchsize,\n",
    "        \"dataroot\": args.dataroot,\n",
    "        \"snapshot\": args.snapshot,\n",
    "        \"resume\": args.resume,\n",
    "        \"architecture\": \"EarlyRNN\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"criterion\": \"EarlyRewardLoss\",\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get doys dict test\n",
      "get doys dict test done\n",
      "get test and validation data...\n",
      "2559635960 2559635960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 178613/178613 [01:16<00:00, 2343.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253658856 2253658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 140645/140645 [01:05<00:00, 2133.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493572704 2493572704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 166391/166391 [01:18<00:00, 2107.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['barley' 'wheat' 'rapeseed' 'corn' 'sunflower' 'orchards' 'nuts'\n",
      " 'permanent meadows' 'temporary meadows']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- LOAD DATASET -----------------------------\n",
    "\n",
    "if args.dataset == \"bavariancrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"bavariancrops\")\n",
    "    nclasses = 7\n",
    "    input_dim = 13\n",
    "    class_weights = None\n",
    "    train_ds = BavarianCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BavarianCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset == \"unitedstates\":\n",
    "    args.dataroot = \"/data/modiscdl/\"\n",
    "    args.sequencelength = 24\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 8\n",
    "    input_dim = 1\n",
    "    train_ds = ModisCDL(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = ModisCDL(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "elif args.dataset == \"breizhcrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"breizhcrops\")\n",
    "    nclasses = 9\n",
    "    input_dim = 13\n",
    "    print(\"get doys dict test\")\n",
    "    doys_dict_test = get_doys_dict_test(dataroot=os.path.join(args.dataroot,args.dataset))\n",
    "    length_sorted_doy_dict_test = create_sorted_doys_dict_test(doys_dict_test)\n",
    "    print(\"get doys dict test done\")\n",
    "    print(\"get train and validation data...\")\n",
    "    train_ds = BreizhCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BreizhCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.ds.classname\n",
    "    print(\"class names:\", class_names)\n",
    "elif args.dataset in [\"ghana\"]:\n",
    "    use_s2_only = False\n",
    "    average_pixel = False\n",
    "    max_n_pixels = 50\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    input_dim = 12 if use_s2_only else 19  # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    args.sequencelength = 365\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength,\n",
    "                                    country=\"ghana\",\n",
    "                                    use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                    max_n_pixels=max_n_pixels)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "\n",
    "    test_ds = SustainbenchCrops(root=dataroot,partition=\"test\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset in [\"southsudan\"]:\n",
    "    use_s2_only = False\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    args.sequencelength = 365\n",
    "    input_dim = 12 if use_s2_only else 19 # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "    test_ds = SustainbenchCrops(root=dataroot, partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    class_names = test_ds.classes\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"dataset {args.dataset} not recognized\")\n",
    "\n",
    "traindataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batchsize)\n",
    "testdataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=args.batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- VISUALIZATION: label distribution -----------------------------\n",
    "# datasets = [train_ds, test_ds]\n",
    "# sets_labels = [\"Train\", \"Validation\"]\n",
    "# fig, ax = plt.subplots(figsize=(15, 7))\n",
    "# fig, ax = plot_label_distribution_datasets(datasets, sets_labels, fig, ax, title='Label distribution', labels_names=class_names)\n",
    "# wandb.log({\"label_distribution\": wandb.Image(fig)})\n",
    "# plt.close(fig)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 121,764 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- SET UP MODEL -----------------------------\n",
    "#model = EarlyRNN(nclasses=nclasses, input_dim=input_dim, hidden_dims=64, sequencelength=args.sequencelength).to(args.device)\n",
    "model = EarlyRNN(\"TempCNN\", nclasses=nclasses, input_dim=input_dim, sequencelength=args.sequencelength, kernel_size=7).to(args.device)\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters.\")\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "# exclude decision head linear bias from weight decay\n",
    "decay, no_decay = list(), list()\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"stopping_decision_head.projection.0.bias\":\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([{'params': no_decay, 'weight_decay': 0, \"lr\": args.learning_rate}, {'params': decay}],\n",
    "                                lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "criterion = EarlyRewardLoss(alpha=args.alpha, epsilon=args.epsilon)\n",
    "\n",
    "if args.resume and os.path.exists(args.snapshot):\n",
    "    model.load_state_dict(torch.load(args.snapshot, map_location=args.device))\n",
    "    optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                        os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                        )\n",
    "    optimizer.load_state_dict(torch.load(optimizer_snapshot, map_location=args.device))\n",
    "    df = pd.read_csv(args.snapshot + \".csv\")\n",
    "    train_stats = df.to_dict(\"records\")\n",
    "    start_epoch = train_stats[-1][\"epoch\"]\n",
    "    print(f\"resuming from {args.snapshot} epoch {start_epoch}\")\n",
    "else:\n",
    "    train_stats = []\n",
    "    start_epoch = 1\n",
    "\n",
    "not_improved = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: trainloss 2.66, testloss 2.93, accuracy 0.75, earliness 0.82. classification loss 9.48, earliness reward 3.61, harmonic mean 0.78. :  20%|██        | 2/10 [26:04<1:44:18, 782.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m      5\u001b[0m     trainloss \u001b[38;5;241m=\u001b[39m train_epoch(model, traindataloader, optimizer, criterion, device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m----> 6\u001b[0m     testloss, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtest_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# statistic logging and visualization...\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     precision, recall, fscore, support \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mprecision_recall_fscore_support(\n\u001b[0;32m     10\u001b[0m         y_pred\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions_at_t_stop\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m0\u001b[39m], y_true\u001b[38;5;241m=\u001b[39mstats[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m0\u001b[39m], average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\utils\\helpers_training.py:70\u001b[0m, in \u001b[0;36mtest_epoch\u001b[1;34m(model, dataloader, criterion, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     69\u001b[0m slengths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 70\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\data\\breizhcrops.py:48\u001b[0m, in \u001b[0;36mBreizhCrops.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     45\u001b[0m     idxs\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m     46\u001b[0m     X \u001b[38;5;241m=\u001b[39m X[idxs]\n\u001b[1;32m---> 48\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m X,y  \u001b[38;5;241m=\u001b[39m X, y\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequencelength)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_id:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------- TRAINING -----------------------------\n",
    "print(\"starting training...\")\n",
    "with tqdm(range(start_epoch, args.epochs + 1)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=args.device)\n",
    "        testloss, stats = test_epoch(model, testdataloader, criterion, args.device)\n",
    "\n",
    "        # statistic logging and visualization...\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0], average=\"macro\",\n",
    "            zero_division=0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0])\n",
    "        kappa = sklearn.metrics.cohen_kappa_score(\n",
    "            stats[\"predictions_at_t_stop\"][:, 0], stats[\"targets\"][:, 0])\n",
    "\n",
    "        classification_loss = stats[\"classification_loss\"].mean()\n",
    "        earliness_reward = stats[\"earliness_reward\"].mean()\n",
    "        earliness = 1 - (stats[\"t_stop\"].mean() / (args.sequencelength - 1))\n",
    "        harmonic_mean = harmonic_mean_score(accuracy, stats[\"classification_earliness\"])\n",
    "\n",
    "        # ----------------------------- LOGGING -----------------------------\n",
    "        train_stats.append(\n",
    "            dict(\n",
    "                epoch=epoch,\n",
    "                trainloss=trainloss,\n",
    "                testloss=testloss,\n",
    "                accuracy=accuracy,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                fscore=fscore,\n",
    "                kappa=kappa,\n",
    "                elects_earliness=earliness,\n",
    "                classification_loss=classification_loss,\n",
    "                earliness_reward=earliness_reward,\n",
    "                classification_earliness=stats[\"classification_earliness\"],\n",
    "                harmonic_mean=harmonic_mean,\n",
    "            )\n",
    "        )\n",
    "        fig_boxplot, ax_boxplot = plt.subplots(figsize=(15, 7))\n",
    "        doys_dict = get_approximated_doys_dict(stats[\"seqlengths\"], length_sorted_doy_dict_test)\n",
    "        doys_stop = get_doy_stop(stats, doys_dict)\n",
    "        fig_boxplot, _ = boxplot_stopping_times(doys_stop, stats, fig_boxplot, ax_boxplot, class_names)\n",
    "        wandb.log({\n",
    "                \"loss\": {\"trainloss\": trainloss, \"testloss\": testloss},\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"fscore\": fscore,\n",
    "                \"kappa\": kappa,\n",
    "                \"elects_earliness\": earliness,\n",
    "                \"classification_loss\": classification_loss,\n",
    "                \"earliness_reward\": earliness_reward,\n",
    "                \"classification_earliness\": stats[\"classification_earliness\"],\n",
    "                \"harmonic_mean\": harmonic_mean,\n",
    "                \"boxplot\": wandb.Image(fig_boxplot),\n",
    "                \"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=stats[\"targets\"][:,0], preds=stats[\"predictions_at_t_stop\"][:,0],\n",
    "                        class_names=class_names, title=\"Confusion Matrix\")\n",
    "            })\n",
    "        plt.close(fig_boxplot)\n",
    "\n",
    "        df = pd.DataFrame(train_stats).set_index(\"epoch\")\n",
    "\n",
    "        savemsg = \"\"\n",
    "        if len(df) > 2:\n",
    "            if testloss < df.testloss[:-1].values.min():\n",
    "                savemsg = f\"saving model to {args.snapshot}\"\n",
    "                os.makedirs(os.path.dirname(args.snapshot), exist_ok=True)\n",
    "                torch.save(model.state_dict(), args.snapshot)\n",
    "\n",
    "                optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                                    os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                                    )\n",
    "                torch.save(optimizer.state_dict(), optimizer_snapshot)\n",
    "                wandb.log_artifact(args.snapshot, type=\"model\")  \n",
    "\n",
    "                df.to_csv(args.snapshot + \".csv\")\n",
    "                not_improved = 0 # reset early stopping counter\n",
    "            else:\n",
    "                not_improved += 1 # increment early stopping counter\n",
    "                if args.patience is not None:\n",
    "                    savemsg = f\"early stopping in {args.patience - not_improved} epochs.\"\n",
    "                else:\n",
    "                    savemsg = \"\"\n",
    "\n",
    "        pbar.set_description(f\"epoch {epoch}: trainloss {trainloss:.2f}, testloss {testloss:.2f}, \"\n",
    "                     f\"accuracy {accuracy:.2f}, earliness {earliness:.2f}. \"\n",
    "                     f\"classification loss {classification_loss:.2f}, earliness reward {earliness_reward:.2f}, harmonic mean {harmonic_mean:.2f}. {savemsg}\")\n",
    "        \n",
    "            \n",
    "        if args.patience is not None:\n",
    "            if not_improved > args.patience:\n",
    "                print(f\"stopping training. testloss {testloss:.2f} did not improve in {args.patience} epochs.\")\n",
    "                break\n",
    "    \n",
    "# ----------------------------- SAVE FINAL MODEL -----------------------------\n",
    "wandb.log_artifact(args.snapshot, type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbdfdaa38b84d6988fbaaabee716ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elects_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
