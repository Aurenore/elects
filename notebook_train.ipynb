{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Notebook - for developing code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "os.environ['MPLCONFIGDIR'] = '/myhome'\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "from data import BavarianCrops, BreizhCrops, SustainbenchCrops, ModisCDL\n",
    "from torch.utils.data import DataLoader\n",
    "from earlyrnn import EarlyRNN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from loss import EarlyRewardLoss\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from utils.plots import plot_label_distribution_datasets, boxplot_stopping_times\n",
    "from utils.doy import get_doys_dict_test, get_doy_stop, create_sorted_doys_dict_test, get_approximated_doys_dict\n",
    "from utils.helpers_training import parse_args, train_epoch, test_epoch\n",
    "from utils.metrics import harmonic_mean_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Run ELECTS Early Classification training on the BavarianCrops dataset.')\n",
    "    parser.add_argument('--dataset', type=str, default=\"bavariancrops\", choices=[\"bavariancrops\",\"breizhcrops\", \"ghana\", \"southsudan\",\"unitedstates\"], help=\"dataset\")\n",
    "    parser.add_argument('--alpha', type=float, default=0.5, help=\"trade-off parameter of earliness and accuracy (eq 6): \"\n",
    "                                                                 \"1=full weight on accuracy; 0=full weight on earliness\")\n",
    "    parser.add_argument('--epsilon', type=float, default=10, help=\"additive smoothing parameter that helps the \"\n",
    "                                                                  \"model recover from too early classifications (eq 7)\")\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-3, help=\"Optimizer learning rate\")\n",
    "    parser.add_argument('--weight-decay', type=float, default=0, help=\"weight_decay\")\n",
    "    parser.add_argument('--patience', type=int, default=30, help=\"Early stopping patience\")\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                        choices=[\"cuda\", \"cpu\"], help=\"'cuda' (GPU) or 'cpu' device to run the code. \"\n",
    "                                                     \"defaults to 'cuda' if GPU is available, otherwise 'cpu'\")\n",
    "    parser.add_argument('--epochs', type=int, default=100, help=\"number of training epochs\")\n",
    "    parser.add_argument('--sequencelength', type=int, default=70, help=\"sequencelength of the time series. If samples are shorter, \"\n",
    "                                                                \"they are zero-padded until this length; \"\n",
    "                                                                \"if samples are longer, they will be undersampled\")\n",
    "    parser.add_argument('--batchsize', type=int, default=256, help=\"number of samples per batch\")\n",
    "    parser.add_argument('--dataroot', type=str, default=os.path.join(os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\")),\"elects_data\"), help=\"directory to download the \"\n",
    "                                                                                 \"BavarianCrops dataset (400MB).\"\n",
    "                                                                                 \"Defaults to home directory.\")\n",
    "    parser.add_argument('--snapshot', type=str, default=\"snapshots/model.pth\",\n",
    "                        help=\"pytorch state dict snapshot file\")\n",
    "    parser.add_argument('--resume', action='store_true')\n",
    "\n",
    "    if args is not None:\n",
    "        args = parser.parse_args(args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if args.patience < 0:\n",
    "        args.patience = None\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available:  cuda\n",
      "Namespace(dataset='breizhcrops', alpha=0.5, epsilon=10, learning_rate=0.001, weight_decay=0, patience=30, device='cuda', epochs=10, sequencelength=70, batchsize=256, dataroot='C:\\\\Users\\\\anyam\\\\elects_data', snapshot='snapshots/model.pth', resume=False)\n"
     ]
    }
   ],
   "source": [
    "# Example of how to call parse_args with custom arguments in a notebook\n",
    "custom_args = \"--dataset breizhcrops --epochs 10\".split()\n",
    "args = parse_args(custom_args)\n",
    "print(\"cuda is available: \", args.device)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: aurenore. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\wandb\\run-20240326_164421-em3l7jdd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aurenore/ELECTS/runs/em3l7jdd/workspace' target=\"_blank\">radiant-armadillo-72</a></strong> to <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aurenore/ELECTS' target=\"_blank\">https://wandb.ai/aurenore/ELECTS</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aurenore/ELECTS/runs/em3l7jdd/workspace' target=\"_blank\">https://wandb.ai/aurenore/ELECTS/runs/em3l7jdd/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aurenore/ELECTS/runs/em3l7jdd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1cc08283850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = \"/mydata/studentanya/anya/wandb/\"\n",
    "wandb.init(\n",
    "        dir=None,\n",
    "        project=\"ELECTS\",\n",
    "        notes=\"first experimentations with ELECTS\",\n",
    "        tags=[\"ELECTS\", args.dataset, \"with_doys_boxplot\"],\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"dataset\": args.dataset,\n",
    "        \"alpha\": args.alpha,\n",
    "        \"epsilon\": args.epsilon,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "        \"patience\": args.patience,\n",
    "        \"device\": args.device,\n",
    "        \"epochs\": args.epochs,\n",
    "        \"sequencelength\": args.sequencelength,\n",
    "        \"batchsize\": args.batchsize,\n",
    "        \"dataroot\": args.dataroot,\n",
    "        \"snapshot\": args.snapshot,\n",
    "        \"resume\": args.resume,\n",
    "        \"architecture\": \"EarlyRNN\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"criterion\": \"EarlyRewardLoss\",\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get doys dict test\n",
      "get doys dict test done\n",
      "get test and validation data...\n",
      "2559635960 2559635960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 178613/178613 [01:09<00:00, 2583.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253658856 2253658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 140645/140645 [00:57<00:00, 2448.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493572704 2493572704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 166391/166391 [01:07<00:00, 2462.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['barley' 'wheat' 'rapeseed' 'corn' 'sunflower' 'orchards' 'nuts'\n",
      " 'permanent meadows' 'temporary meadows']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- LOAD DATASET -----------------------------\n",
    "\n",
    "if args.dataset == \"bavariancrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"bavariancrops\")\n",
    "    nclasses = 7\n",
    "    input_dim = 13\n",
    "    class_weights = None\n",
    "    train_ds = BavarianCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BavarianCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset == \"unitedstates\":\n",
    "    args.dataroot = \"/data/modiscdl/\"\n",
    "    args.sequencelength = 24\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 8\n",
    "    input_dim = 1\n",
    "    train_ds = ModisCDL(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = ModisCDL(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "elif args.dataset == \"breizhcrops\":\n",
    "    dataroot = os.path.join(args.dataroot,\"breizhcrops\")\n",
    "    nclasses = 9\n",
    "    input_dim = 13\n",
    "    print(\"get doys dict test\")\n",
    "    doys_dict_test = get_doys_dict_test(dataroot=os.path.join(args.dataroot,args.dataset))\n",
    "    length_sorted_doy_dict_test = create_sorted_doys_dict_test(doys_dict_test)\n",
    "    print(\"get doys dict test done\")\n",
    "    print(\"get test and validation data...\")\n",
    "    train_ds = BreizhCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength)\n",
    "    test_ds = BreizhCrops(root=dataroot,partition=\"valid\", sequencelength=args.sequencelength)\n",
    "    class_names = test_ds.ds.classname\n",
    "    print(\"class names:\", class_names)\n",
    "elif args.dataset in [\"ghana\"]:\n",
    "    use_s2_only = False\n",
    "    average_pixel = False\n",
    "    max_n_pixels = 50\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    input_dim = 12 if use_s2_only else 19  # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    args.sequencelength = 365\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength,\n",
    "                                    country=\"ghana\",\n",
    "                                    use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                    max_n_pixels=max_n_pixels)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "\n",
    "    test_ds = SustainbenchCrops(root=dataroot,partition=\"test\", sequencelength=args.sequencelength,\n",
    "                                country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                max_n_pixels=max_n_pixels)\n",
    "    class_names = test_ds.classes\n",
    "elif args.dataset in [\"southsudan\"]:\n",
    "    use_s2_only = False\n",
    "    dataroot = args.dataroot\n",
    "    nclasses = 4\n",
    "    args.sequencelength = 365\n",
    "    input_dim = 12 if use_s2_only else 19 # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "    args.epochs = 500\n",
    "    train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=args.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "\n",
    "    train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "    test_ds = SustainbenchCrops(root=dataroot, partition=\"val\", sequencelength=args.sequencelength,\n",
    "                                country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "    class_names = test_ds.classes\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"dataset {args.dataset} not recognized\")\n",
    "\n",
    "traindataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=args.batchsize)\n",
    "testdataloader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=args.batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting labels from dataset Train.\n",
      "Extracting labels from dataset Validation.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- VISUALIZATION: label distribution -----------------------------\n",
    "datasets = [train_ds, test_ds]\n",
    "sets_labels = [\"Train\", \"Validation\"]\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "fig, ax = plot_label_distribution_datasets(datasets, sets_labels, fig, ax, title='Label distribution', labels_names=class_names)\n",
    "wandb.log({\"label_distribution\": wandb.Image(fig)})\n",
    "plt.close(fig)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- SET UP MODEL -----------------------------\n",
    "model = EarlyRNN(nclasses=nclasses, input_dim=input_dim).to(args.device)\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "# exclude decision head linear bias from weight decay\n",
    "decay, no_decay = list(), list()\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"stopping_decision_head.projection.0.bias\":\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([{'params': no_decay, 'weight_decay': 0, \"lr\": args.learning_rate}, {'params': decay}],\n",
    "                                lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "criterion = EarlyRewardLoss(alpha=args.alpha, epsilon=args.epsilon)\n",
    "\n",
    "if args.resume and os.path.exists(args.snapshot):\n",
    "    model.load_state_dict(torch.load(args.snapshot, map_location=args.device))\n",
    "    optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                        os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                        )\n",
    "    optimizer.load_state_dict(torch.load(optimizer_snapshot, map_location=args.device))\n",
    "    df = pd.read_csv(args.snapshot + \".csv\")\n",
    "    train_stats = df.to_dict(\"records\")\n",
    "    start_epoch = train_stats[-1][\"epoch\"]\n",
    "    print(f\"resuming from {args.snapshot} epoch {start_epoch}\")\n",
    "else:\n",
    "    train_stats = []\n",
    "    start_epoch = 1\n",
    "\n",
    "not_improved = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harmonic mean 0.7225970558877745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: trainloss 1.20, testloss 1.63, accuracy 0.78, earliness 0.70. classification loss 6.94, earliness reward 3.68, harmonic mean 0.72. saving model to snapshots/model.pth:  10%|█         | 1/10 [09:39<1:26:55, 579.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m----> 5\u001b[0m         trainloss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraindataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         testloss, stats \u001b[38;5;241m=\u001b[39m test_epoch(model, testdataloader, criterion, args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# statistic logging and visualization...\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\utils\\helpers_training.py:46\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 46\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\data\\breizhcrops.py:48\u001b[0m, in \u001b[0;36mBreizhCrops.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     45\u001b[0m     idxs\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m     46\u001b[0m     X \u001b[38;5;241m=\u001b[39m X[idxs]\n\u001b[1;32m---> 48\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m X,y  \u001b[38;5;241m=\u001b[39m X, y\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequencelength)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_id:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------- TRAINING -----------------------------\n",
    "print(\"starting training...\")\n",
    "with tqdm(range(start_epoch, args.epochs + 1)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=args.device)\n",
    "        testloss, stats = test_epoch(model, testdataloader, criterion, args.device)\n",
    "\n",
    "        # statistic logging and visualization...\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0], average=\"macro\",\n",
    "            zero_division=0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0])\n",
    "        kappa = sklearn.metrics.cohen_kappa_score(\n",
    "            stats[\"predictions_at_t_stop\"][:, 0], stats[\"targets\"][:, 0])\n",
    "\n",
    "        classification_loss = stats[\"classification_loss\"].mean()\n",
    "        earliness_reward = stats[\"earliness_reward\"].mean()\n",
    "        earliness = 1 - (stats[\"t_stop\"].mean() / (args.sequencelength - 1))\n",
    "        harmonic_mean = harmonic_mean_score(accuracy, stats[\"classification_earliness\"])\n",
    "\n",
    "        # ----------------------------- LOGGING -----------------------------\n",
    "        train_stats.append(\n",
    "            dict(\n",
    "                epoch=epoch,\n",
    "                trainloss=trainloss,\n",
    "                testloss=testloss,\n",
    "                accuracy=accuracy,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                fscore=fscore,\n",
    "                kappa=kappa,\n",
    "                elects_earliness=earliness,\n",
    "                classification_loss=classification_loss,\n",
    "                earliness_reward=earliness_reward,\n",
    "                classification_earliness=stats[\"classification_earliness\"],\n",
    "                harmonic_mean=harmonic_mean,\n",
    "            )\n",
    "        )\n",
    "        fig_boxplot, ax_boxplot = plt.subplots(figsize=(15, 7))\n",
    "        doys_dict = get_approximated_doys_dict(stats[\"seqlengths\"], length_sorted_doy_dict_test)\n",
    "        doys_stop = get_doy_stop(stats, doys_dict)\n",
    "        fig_boxplot, _ = boxplot_stopping_times(doys_stop, stats, fig_boxplot, ax_boxplot, class_names)\n",
    "        wandb.log({\n",
    "                \"loss\": {\"trainloss\": trainloss, \"testloss\": testloss},\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"fscore\": fscore,\n",
    "                \"kappa\": kappa,\n",
    "                \"elects_earliness\": earliness,\n",
    "                \"classification_loss\": classification_loss,\n",
    "                \"earliness_reward\": earliness_reward,\n",
    "                \"classification_earliness\": stats[\"classification_earliness\"],\n",
    "                \"harmonic_mean\": harmonic_mean,\n",
    "                \"boxplot\": wandb.Image(fig_boxplot),\n",
    "                \"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=stats[\"targets\"][:,0], preds=stats[\"predictions_at_t_stop\"][:,0],\n",
    "                        class_names=class_names, title=\"Confusion Matrix\")\n",
    "            })\n",
    "        plt.close(fig_boxplot)\n",
    "\n",
    "        df = pd.DataFrame(train_stats).set_index(\"epoch\")\n",
    "\n",
    "        savemsg = \"\"\n",
    "        if len(df) > 2:\n",
    "            if testloss < df.testloss[:-1].values.min():\n",
    "                savemsg = f\"saving model to {args.snapshot}\"\n",
    "                os.makedirs(os.path.dirname(args.snapshot), exist_ok=True)\n",
    "                torch.save(model.state_dict(), args.snapshot)\n",
    "\n",
    "                optimizer_snapshot = os.path.join(os.path.dirname(args.snapshot),\n",
    "                                                    os.path.basename(args.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                                    )\n",
    "                torch.save(optimizer.state_dict(), optimizer_snapshot)\n",
    "                wandb.log_artifact(args.snapshot, type=\"model\")  \n",
    "\n",
    "                df.to_csv(args.snapshot + \".csv\")\n",
    "                not_improved = 0 # reset early stopping counter\n",
    "            else:\n",
    "                not_improved += 1 # increment early stopping counter\n",
    "                if args.patience is not None:\n",
    "                    savemsg = f\"early stopping in {args.patience - not_improved} epochs.\"\n",
    "                else:\n",
    "                    savemsg = \"\"\n",
    "\n",
    "        pbar.set_description(f\"epoch {epoch}: trainloss {trainloss:.2f}, testloss {testloss:.2f}, \"\n",
    "                     f\"accuracy {accuracy:.2f}, earliness {earliness:.2f}. \"\n",
    "                     f\"classification loss {classification_loss:.2f}, earliness reward {earliness_reward:.2f}, harmonic mean {harmonic_mean:.2f}. {savemsg}\")\n",
    "        \n",
    "            \n",
    "        if args.patience is not None:\n",
    "            if not_improved > args.patience:\n",
    "                print(f\"stopping training. testloss {testloss:.2f} did not improve in {args.patience} epochs.\")\n",
    "                break\n",
    "    \n",
    "# ----------------------------- SAVE FINAL MODEL -----------------------------\n",
    "wandb.log_artifact(args.snapshot, type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elects_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
