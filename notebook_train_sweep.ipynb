{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "os.environ['MPLCONFIGDIR'] = '/myhome'\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "from data import BavarianCrops, BreizhCrops, SustainbenchCrops, ModisCDL\n",
    "from torch.utils.data import DataLoader\n",
    "from earlyrnn import EarlyRNN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from loss import EarlyRewardLoss\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from utils.plots import plot_label_distribution_datasets, boxplot_stopping_times\n",
    "from utils.doy import get_doys_dict_test, get_doy_stop, create_sorted_doys_dict_test, get_approximated_doys_dict\n",
    "from utils.helpers_training import parse_args_sweep, train_epoch\n",
    "from utils.helpers_testing import test_epoch\n",
    "from utils.metrics import harmonic_mean_score\n",
    "from models.model_helpers import count_parameters\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init sweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: aurenore. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: rjqfsib0\n",
      "Sweep URL: https://wandb.ai/aurenore/MasterThesis/sweeps/rjqfsib0\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "PROJECTUSER_PATH=\"/mydata/studentanya/anya\"\n",
    "DATA=\"breizhcrops\"\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"method\": \"random\",\n",
    "    \"name\": \"sweep_class_weights\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"harmonic_mean\"},\n",
    "    \"parameters\": {\n",
    "        \"backbonemodel\": {\"value\": \"TempCNN\"},\n",
    "        \"dataset\": {\"value\": DATA},\n",
    "        \"alpha\": {\"value\": 0.5},\n",
    "        \"epsilon\": {\"value\": 10},\n",
    "        \"learning_rate\": {\"value\": 1e-3},\n",
    "        \"weight_decay\": {\"value\": 0},\n",
    "        \"patience\": {\"value\": 30},\n",
    "        \"device\": {\"value\": \"cuda\"},\n",
    "        \"epochs\": {\"value\": 100},\n",
    "        \"sequencelength\": {\"value\": 70},\n",
    "        \"extra_padding_list\": {\"value\": [50, 40, 30, 20, 10, 0]},\n",
    "        \"hidden_dims\": {\"values\": [16, 32, 64]},\n",
    "        \"batchsize\": {\"value\": 256},\n",
    "        \"dataroot\": {\"value\": os.path.join(os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\")),\"elects_data\")},\n",
    "        \"snapshot\": {\"value\": \"snapshots/model.pth\"},\n",
    "        \"left_padding\": {\"values\": [True, False]},\n",
    "        \"loss_weight\": {\"values\": [None, \"balanced\"]},\n",
    "        \"resume\": {\"value\": False},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"MasterThesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start sweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWEEP_ID =sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: mymt585n with config:\n",
      "wandb: \talpha: 0.5\n",
      "wandb: \tbackbonemodel: TempCNN\n",
      "wandb: \tbatchsize: 256\n",
      "wandb: \tdataroot: C:\\Users\\anyam\\elects_data\n",
      "wandb: \tdataset: breizhcrops\n",
      "wandb: \tdevice: cuda\n",
      "wandb: \tepochs: 100\n",
      "wandb: \tepsilon: 10\n",
      "wandb: \textra_padding_list: [50, 40, 30, 20, 10, 0]\n",
      "wandb: \thidden_dims: 64\n",
      "wandb: \tlearning_rate: 0.001\n",
      "wandb: \tleft_padding: True\n",
      "wandb: \tpatience: 30\n",
      "wandb: \tresume: False\n",
      "wandb: \tsequencelength: 70\n",
      "wandb: \tsnapshot: snapshots/model.pth\n",
      "wandb: \tweight_decay: 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: WARNING Ignored wandb.init() arg project when running a sweep.\n",
      "wandb: WARNING Path /mydata/studentanya/anya/wandb/wandb\\ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\anyam\\AppData\\Local\\Temp\\wandb\\run-20240424_155522-mymt585n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aurenore/MasterThesis/runs/mymt585n/workspace' target=\"_blank\">lemon-sweep-1</a></strong> to <a href='https://wandb.ai/aurenore/MasterThesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/aurenore/MasterThesis/sweeps/ofo0x46n' target=\"_blank\">https://wandb.ai/aurenore/MasterThesis/sweeps/ofo0x46n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aurenore/MasterThesis' target=\"_blank\">https://wandb.ai/aurenore/MasterThesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/aurenore/MasterThesis/sweeps/ofo0x46n' target=\"_blank\">https://wandb.ai/aurenore/MasterThesis/sweeps/ofo0x46n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aurenore/MasterThesis/runs/mymt585n/workspace' target=\"_blank\">https://wandb.ai/aurenore/MasterThesis/runs/mymt585n/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get train and validation data...\n",
      "2559635960 2559635960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 178613/178613 [01:14<00:00, 2392.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253658856 2253658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 140645/140645 [00:58<00:00, 2412.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493572704 2493572704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 166391/166391 [01:09<00:00, 2377.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class names: ['barley' 'wheat' 'rapeseed' 'corn' 'sunflower' 'orchards' 'nuts'\n",
      " 'permanent meadows' 'temporary meadows']\n",
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: trainloss 4.53, testloss 4.83, accuracy 0.72, earliness 0.87. classification loss 11.86, earliness reward 2.21, harmonic mean 0.78. :   1%|          | 1/100 [19:05<31:29:49, 1145.35s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    wandb.init(\n",
    "        dir=\"/mydata/studentanya/anya/wandb/\",\n",
    "        notes=\"ELECTS with different backbone models.\",\n",
    "        tags=[\"ELECTS\", \"earlyrnn\", \"trials\", \"sweep\"],\n",
    "    )\n",
    "    config = wandb.config\n",
    "    # ----------------------------- LOAD DATASET -----------------------------\n",
    "\n",
    "    if config.dataset == \"bavariancrops\":\n",
    "        dataroot = os.path.join(config.dataroot,\"bavariancrops\")\n",
    "        nclasses = 7\n",
    "        input_dim = 13\n",
    "        class_weights = None\n",
    "        train_ds = BavarianCrops(root=dataroot,partition=\"train\", sequencelength=config.sequencelength)\n",
    "        test_ds = BavarianCrops(root=dataroot,partition=\"valid\", sequencelength=config.sequencelength)\n",
    "        class_names = test_ds.classes\n",
    "    elif config.dataset == \"unitedstates\":\n",
    "        config.dataroot = \"/data/modiscdl/\"\n",
    "        config.sequencelength = 24\n",
    "        dataroot = config.dataroot\n",
    "        nclasses = 8\n",
    "        input_dim = 1\n",
    "        train_ds = ModisCDL(root=dataroot,partition=\"train\", sequencelength=config.sequencelength)\n",
    "        test_ds = ModisCDL(root=dataroot,partition=\"valid\", sequencelength=config.sequencelength)\n",
    "    elif config.dataset == \"breizhcrops\":\n",
    "        dataroot = os.path.join(config.dataroot,\"breizhcrops\")\n",
    "        nclasses = 9\n",
    "        input_dim = 13\n",
    "        doys_dict_test = get_doys_dict_test(dataroot=os.path.join(config.dataroot,config.dataset))\n",
    "        length_sorted_doy_dict_test = create_sorted_doys_dict_test(doys_dict_test)\n",
    "        print(\"get train and validation data...\")\n",
    "        train_ds = BreizhCrops(root=dataroot,partition=\"train\", sequencelength=config.sequencelength)\n",
    "        test_ds = BreizhCrops(root=dataroot,partition=\"valid\", sequencelength=config.sequencelength)\n",
    "        class_names = test_ds.ds.classname\n",
    "        print(\"class names:\", class_names)\n",
    "    elif config.dataset in [\"ghana\"]:\n",
    "        use_s2_only = False\n",
    "        average_pixel = False\n",
    "        max_n_pixels = 50\n",
    "        dataroot = config.dataroot\n",
    "        nclasses = 4\n",
    "        input_dim = 12 if use_s2_only else 19  # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "        config.epochs = 500\n",
    "        config.sequencelength = 365\n",
    "        train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=config.sequencelength,\n",
    "                                     country=\"ghana\",\n",
    "                                     use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                     max_n_pixels=max_n_pixels)\n",
    "        val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=config.sequencelength,\n",
    "                                    country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                    max_n_pixels=max_n_pixels)\n",
    "\n",
    "        train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "\n",
    "        test_ds = SustainbenchCrops(root=dataroot,partition=\"test\", sequencelength=config.sequencelength,\n",
    "                                    country=\"ghana\", use_s2_only=use_s2_only, average_pixel=average_pixel,\n",
    "                                    max_n_pixels=max_n_pixels)\n",
    "        class_names = test_ds.classes\n",
    "    elif config.dataset in [\"southsudan\"]:\n",
    "        use_s2_only = False\n",
    "        dataroot = config.dataroot\n",
    "        nclasses = 4\n",
    "        config.sequencelength = 365\n",
    "        input_dim = 12 if use_s2_only else 19 # 12 sentinel 2 + 3 x sentinel 1 + 4 * planet\n",
    "        config.epochs = 500\n",
    "        train_ds = SustainbenchCrops(root=dataroot,partition=\"train\", sequencelength=config.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "        val_ds = SustainbenchCrops(root=dataroot,partition=\"val\", sequencelength=config.sequencelength, country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "\n",
    "        train_ds = torch.utils.data.ConcatDataset([train_ds, val_ds])\n",
    "        test_ds = SustainbenchCrops(root=dataroot, partition=\"val\", sequencelength=config.sequencelength,\n",
    "                                   country=\"southsudan\", use_s2_only=use_s2_only)\n",
    "        class_names = test_ds.classes\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"dataset {config.dataset} not recognized\")\n",
    "    \n",
    "    traindataloader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config.batchsize)\n",
    "    testdataloader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=config.batchsize)\n",
    "    # ----------------------------- VISUALIZATION: label distribution -----------------------------\n",
    "    # datasets = [train_ds, test_ds]\n",
    "    # sets_labels = [\"Train\", \"Validation\"]\n",
    "    # fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    # fig, ax = plot_label_distribution_datasets(datasets, sets_labels, fig, ax, title='Label distribution', labels_names=class_names)\n",
    "    # wandb.log({\"label_distribution\": wandb.Image(fig)})\n",
    "    # plt.close(fig)\n",
    "        \n",
    "    # ----------------------------- SET UP MODEL -----------------------------\n",
    "    model = EarlyRNN(config.backbonemodel, nclasses=nclasses, input_dim=input_dim, sequencelength=config.sequencelength, hidden_dims=config.hidden_dims, left_padding=config.left_padding).to(config.device)\n",
    "    wandb.config.update({\"nb_parameters\": count_parameters(model)})\n",
    "\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "    # exclude decision head linear bias from weight decay\n",
    "    decay, no_decay = list(), list()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name == \"stopping_decision_head.projection.0.bias\":\n",
    "            no_decay.append(param)\n",
    "        else:\n",
    "            decay.append(param)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([{'params': no_decay, 'weight_decay': 0, \"lr\": config.learning_rate}, {'params': decay}],\n",
    "                                  lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "    if config.loss_weight == \"balanced\":\n",
    "        class_weights = train_ds.get_class_weights().to(config.device)\n",
    "    else: \n",
    "        class_weights = None\n",
    "    criterion = EarlyRewardLoss(alpha=config.alpha, epsilon=config.epsilon, weight=class_weights)   \n",
    "    if config.resume and os.path.exists(config.snapshot):\n",
    "        model.load_state_dict(torch.load(config.snapshot, map_location=config.device))\n",
    "        optimizer_snapshot = os.path.join(os.path.dirname(config.snapshot),\n",
    "                                          os.path.basename(config.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                          )\n",
    "        optimizer.load_state_dict(torch.load(optimizer_snapshot, map_location=config.device))\n",
    "        df = pd.read_csv(config.snapshot + \".csv\")\n",
    "        train_stats = df.to_dict(\"records\")\n",
    "        start_epoch = train_stats[-1][\"epoch\"]\n",
    "        print(f\"resuming from {config.snapshot} epoch {start_epoch}\")\n",
    "    else:\n",
    "        train_stats = []\n",
    "        start_epoch = 1\n",
    "\n",
    "    not_improved = 0\n",
    "    \n",
    "    # ----------------------------- TRAINING -----------------------------\n",
    "    print(\"starting training...\")\n",
    "    with tqdm(range(start_epoch, config.epochs + 1)) as pbar:\n",
    "        for epoch in pbar:\n",
    "            trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=config.device, extra_padding_list=config.extra_padding_list)\n",
    "            testloss, stats = test_epoch(model, testdataloader, criterion, config.device, extra_padding_list=config.extra_padding_list, return_id=test_ds.return_id)\n",
    "\n",
    "            # statistic logging and visualization...\n",
    "            precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "                y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0], average=\"macro\",\n",
    "                zero_division=0)\n",
    "            accuracy = sklearn.metrics.accuracy_score(\n",
    "                y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0])\n",
    "            kappa = sklearn.metrics.cohen_kappa_score(\n",
    "                stats[\"predictions_at_t_stop\"][:, 0], stats[\"targets\"][:, 0])\n",
    "\n",
    "            classification_loss = stats[\"classification_loss\"].mean()\n",
    "            earliness_reward = stats[\"earliness_reward\"].mean()\n",
    "            earliness = 1 - (stats[\"t_stop\"].mean() / (config.sequencelength - 1))\n",
    "            harmonic_mean = harmonic_mean_score(accuracy, stats[\"classification_earliness\"])\n",
    "\n",
    "            # ----------------------------- LOGGING -----------------------------\n",
    "            train_stats.append(\n",
    "                dict(\n",
    "                    epoch=epoch,\n",
    "                    trainloss=trainloss,\n",
    "                    testloss=testloss,\n",
    "                    accuracy=accuracy,\n",
    "                    precision=precision,\n",
    "                    recall=recall,\n",
    "                    fscore=fscore,\n",
    "                    kappa=kappa,\n",
    "                    elects_earliness=earliness,\n",
    "                    classification_loss=classification_loss,\n",
    "                    earliness_reward=earliness_reward,\n",
    "                    classification_earliness=stats[\"classification_earliness\"],\n",
    "                    harmonic_mean=harmonic_mean,\n",
    "                )\n",
    "            )\n",
    "            dict_to_wandb = {\n",
    "                    \"loss\": {\"trainloss\": trainloss, \"testloss\": testloss},\n",
    "                    \"accuracy\": accuracy,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"fscore\": fscore,\n",
    "                    \"kappa\": kappa,\n",
    "                    \"elects_earliness\": earliness,\n",
    "                    \"classification_loss\": classification_loss,\n",
    "                    \"earliness_reward\": earliness_reward,\n",
    "                    \"classification_earliness\": stats[\"classification_earliness\"],\n",
    "                    \"harmonic_mean\": harmonic_mean,\n",
    "                    \"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                            y_true=stats[\"targets\"][:,0], preds=stats[\"predictions_at_t_stop\"][:,0],\n",
    "                            class_names=class_names, title=\"Confusion Matrix\")\n",
    "                }\n",
    "            if epoch % 10 == 1:\n",
    "                fig_boxplot, ax_boxplot = plt.subplots(figsize=(15, 7))\n",
    "                doys_dict = get_approximated_doys_dict(stats[\"seqlengths\"], length_sorted_doy_dict_test)\n",
    "                doys_stop = get_doy_stop(stats, doys_dict)\n",
    "                fig_boxplot, _ = boxplot_stopping_times(doys_stop, stats, fig_boxplot, ax_boxplot, class_names)\n",
    "                dict_to_wandb[\"boxplot\"] = wandb.Image(fig_boxplot)\n",
    "                plt.close(fig_boxplot)\n",
    "            \n",
    "            wandb.log(dict_to_wandb)\n",
    "            \n",
    "\n",
    "            df = pd.DataFrame(train_stats).set_index(\"epoch\")\n",
    "\n",
    "            savemsg = \"\"\n",
    "            if len(df) > 2:\n",
    "                if testloss < df.testloss[:-1].values.min():\n",
    "                    savemsg = f\"saving model to {config.snapshot}\"\n",
    "                    os.makedirs(os.path.dirname(config.snapshot), exist_ok=True)\n",
    "                    torch.save(model.state_dict(), config.snapshot)\n",
    "\n",
    "                    optimizer_snapshot = os.path.join(os.path.dirname(config.snapshot),\n",
    "                                                        os.path.basename(config.snapshot).replace(\".pth\", \"_optimizer.pth\")\n",
    "                                                        )\n",
    "                    torch.save(optimizer.state_dict(), optimizer_snapshot)\n",
    "                    wandb.log_artifact(config.snapshot, type=\"model\")  \n",
    "\n",
    "                    df.to_csv(config.snapshot + \".csv\")\n",
    "                    not_improved = 0 # reset early stopping counter\n",
    "                else:\n",
    "                    not_improved += 1 # increment early stopping counter\n",
    "                    if config.patience is not None:\n",
    "                        savemsg = f\"early stopping in {config.patience - not_improved} epochs.\"\n",
    "                    else:\n",
    "                        savemsg = \"\"\n",
    "\n",
    "            pbar.set_description(f\"epoch {epoch}: trainloss {trainloss:.2f}, testloss {testloss:.2f}, \"\n",
    "                        f\"accuracy {accuracy:.2f}, earliness {earliness:.2f}. \"\n",
    "                        f\"classification loss {classification_loss:.2f}, earliness reward {earliness_reward:.2f}, harmonic mean {harmonic_mean:.2f}. {savemsg}\")\n",
    "            \n",
    "                \n",
    "            if config.patience is not None:\n",
    "                if not_improved > config.patience:\n",
    "                    print(f\"stopping training. testloss {testloss:.2f} did not improve in {config.patience} epochs.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sweep_id = SWEEP_ID\n",
    "    wandb.agent(sweep_id, function=main, count=3, project=\"MasterThesis\")\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
