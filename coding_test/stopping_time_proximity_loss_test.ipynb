{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test daily_earlyrnn model\n",
    "Test the new model to check if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "#os.environ['MPLCONFIGDIR'] = \"$HOME\"\n",
    "#os.envir\n",
    "# on[\"WANDB_DIR\"] = os.path.join(os.path.dirname(__file__), \"..\", \"wandb\")\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "# sys append \n",
    "sys.path.append(\"..\")\n",
    "from data import BavarianCrops, BreizhCrops, SustainbenchCrops, ModisCDL\n",
    "from torch.utils.data import DataLoader\n",
    "from models.earlyrnn import EarlyRNN\n",
    "from models.daily_earlyrnn import DailyEarlyRNN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from utils.losses.early_reward_loss import EarlyRewardLoss\n",
    "from utils.losses.stopping_time_proximity_loss import StoppingTimeProximityLoss, sample_three_uniform_numbers\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from utils.plots import plot_label_distribution_datasets, boxplot_stopping_times\n",
    "from utils.doy import get_doys_dict_test, get_doy_stop, create_sorted_doys_dict_test, get_approximated_doys_dict\n",
    "from utils.helpers_training import parse_args_sweep, train_epoch\n",
    "from utils.helpers_testing import test_epoch\n",
    "from utils.metrics import harmonic_mean_score\n",
    "from models.model_helpers import count_parameters\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.6\n",
    "        self.backbonemodel = \"LSTM\"\n",
    "        self.batchsize = 256\n",
    "        self.corrected = True\n",
    "        self.dataroot = os.path.join(os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\")),\"elects_data\")\n",
    "        self.dataset = \"breizhcrops\"\n",
    "        self.device = \"cuda\"\n",
    "        self.epochs = 100\n",
    "        self.epsilon = 10\n",
    "        self.extra_padding_list = [0]\n",
    "        self.hidden_dims = 64\n",
    "        self.learning_rate = 0.001\n",
    "        self.loss_weight = \"balanced\"\n",
    "        self.patience = 30\n",
    "        self.resume = False\n",
    "        self.sequencelength = 365\n",
    "        self.validation_set = \"valid\"\n",
    "        self.weight_decay = 0\n",
    "        self.daily_timestamps = True\n",
    "        self.original_time_serie_lengths = [102]\n",
    "        self.loss = \"stopping_time_proximity\"\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2559635960 2559635960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 67523/67523 [00:31<00:00, 2148.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253658856 2253658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 85310/85310 [00:35<00:00, 2371.41it/s]\n"
     ]
    }
   ],
   "source": [
    "dataroot = os.path.join(config.dataroot,\"breizhcrops\")\n",
    "input_dim = 13\n",
    "test_ds = BreizhCrops(root=dataroot,partition=config.validation_set, sequencelength=config.sequencelength, corrected=config.corrected, daily_timestamps=config.daily_timestamps, original_time_serie_lengths=config.original_time_serie_lengths)\n",
    "train_ds = BreizhCrops(root=dataroot,partition=\"train\", sequencelength=config.sequencelength, corrected=config.corrected, daily_timestamps=config.daily_timestamps, original_time_serie_lengths=config.original_time_serie_lengths)\n",
    "nclasses = train_ds.nclasses\n",
    "class_names = train_ds.labels_names\n",
    "traindataloader = DataLoader(train_ds,batch_size=config.batchsize)\n",
    "testdataloader = DataLoader(test_ds, batch_size=config.batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alphas: [0.6, 0.2, 0.2, 0.0]\n"
     ]
    }
   ],
   "source": [
    "alpha1, alpha2, alpha3, alpha4 = 0.6, 0.2, 0.2, 0. \n",
    "\n",
    "config.alphas = [alpha1, alpha2, alpha3, alpha4]\n",
    "print(f\"alphas: {config.alphas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DailyEarlyRNN(config.backbonemodel, nclasses=nclasses, input_dim=input_dim, sequencelength=config.sequencelength, hidden_dims=config.hidden_dims).to(config.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude decision head linear bias from weight decay\n",
    "decay, no_decay = list(), list()\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"stopping_decision_head.projection.0.bias\":\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([{'params': no_decay, 'weight_decay': 0, \"lr\": config.learning_rate}, {'params': decay}],\n",
    "                                lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m EarlyRewardLoss(alpha\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39malpha, epsilon\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepsilon, weight\u001b[38;5;241m=\u001b[39mclass_weights)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstopping_time_proximity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m \u001b[43mStoppingTimeProximityLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\coding_test\\..\\utils\\losses\\stopping_time_proximity_loss.py:15\u001b[0m, in \u001b[0;36mStoppingTimeProximityLoss.__init__\u001b[1;34m(self, alphas, weight)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, alphas\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m], weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    INPUT: \u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    - alphas: list of 3 floats that sum to 1, the weights of the classification loss, earliness reward and proximity reward. Must be positive\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    - None\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mStoppingTimeProximityLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_log_likelihood \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight\u001b[38;5;241m=\u001b[39mweight)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas \u001b[38;5;241m=\u001b[39m alphas\n",
      "\u001b[1;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "if config.loss_weight == \"balanced\":\n",
    "    class_weights = train_ds.get_class_weights().to(config.device)\n",
    "else: \n",
    "    class_weights = None\n",
    "\n",
    "if config.loss == \"early_reward\":\n",
    "    criterion = EarlyRewardLoss(alpha=config.alpha, epsilon=config.epsilon, weight=class_weights)\n",
    "elif config.loss == \"stopping_time_proximity\":\n",
    "    criterion = StoppingTimeProximityLoss(alphas=config.alphas, weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch, config\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m----> 6\u001b[0m         trainloss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraindataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         testloss, stats \u001b[38;5;241m=\u001b[39m test_epoch(model, testdataloader, criterion, config\u001b[38;5;241m.\u001b[39mdevice, return_id\u001b[38;5;241m=\u001b[39mtest_ds\u001b[38;5;241m.\u001b[39mreturn_id, daily_timestamps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdaily_timestamps)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# statistic logging and visualization...\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\coding_test\\..\\utils\\helpers_training.py:73\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device, extra_padding_list)\u001b[0m\n\u001b[0;32m     70\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(log_class_probabilities, stopping_criteria, y_true)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loss\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 73\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     76\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------- TRAINING -----------------------------\n",
    "start_epoch = 1\n",
    "print(\"starting training...\")\n",
    "with tqdm(range(start_epoch, config.epochs + 1)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=config.device)\n",
    "        testloss, stats = test_epoch(model, testdataloader, criterion, config.device, return_id=test_ds.return_id, daily_timestamps=config.daily_timestamps)\n",
    "\n",
    "        # statistic logging and visualization...\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0], average=\"macro\",\n",
    "            zero_division=0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0])\n",
    "        kappa = sklearn.metrics.cohen_kappa_score(\n",
    "            stats[\"predictions_at_t_stop\"][:, 0], stats[\"targets\"][:, 0])\n",
    "\n",
    "        classification_loss = stats[\"classification_loss\"].mean()\n",
    "        earliness_reward = stats[\"earliness_reward\"].mean()\n",
    "        earliness = 1 - (stats[\"t_stop\"].mean() / (config.sequencelength - 1))\n",
    "        harmonic_mean = harmonic_mean_score(accuracy, stats[\"classification_earliness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
