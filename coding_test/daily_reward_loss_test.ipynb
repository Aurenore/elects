{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test daily reward loss\n",
    "Test the new model to check if it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "#os.environ['MPLCONFIGDIR'] = \"$HOME\"\n",
    "#os.envir\n",
    "# on[\"WANDB_DIR\"] = os.path.join(os.path.dirname(__file__), \"..\", \"wandb\")\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n",
    "# sys append \n",
    "sys.path.append(\"..\")\n",
    "from data import BavarianCrops, BreizhCrops, SustainbenchCrops, ModisCDL\n",
    "from torch.utils.data import DataLoader\n",
    "from models.earlyrnn import EarlyRNN\n",
    "from models.daily_earlyrnn import DailyEarlyRNN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from utils.losses.early_reward_loss import EarlyRewardLoss\n",
    "from utils.losses.stopping_time_proximity_loss import StoppingTimeProximityLoss, sample_three_uniform_numbers\n",
    "from utils.losses.daily_reward_loss import DailyRewardLoss, log_class_prob_at_t_plus_zt\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from utils.plots import plot_label_distribution_datasets, boxplot_stopping_times\n",
    "from utils.doy import get_doys_dict_test, get_doy_stop, create_sorted_doys_dict_test, get_approximated_doys_dict\n",
    "from utils.helpers_training import parse_args_sweep, train_epoch\n",
    "from utils.helpers_testing import test_epoch\n",
    "from utils.metrics import harmonic_mean_score\n",
    "from models.model_helpers import count_parameters\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the helper functions for the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_class_probabilities\n",
      " tensor([[[0.0256, 0.5144, 0.7861, 0.1210],\n",
      "         [0.0609, 0.9291, 0.9235, 0.2702],\n",
      "         [0.3193, 0.5586, 0.1668, 0.7046],\n",
      "         [0.8062, 0.2824, 0.4670, 0.8571],\n",
      "         [0.2979, 0.1941, 0.9765, 0.2255]]])\n",
      "timestamps_left\n",
      " tensor([[1, 1, 0, 1, 1]])\n",
      "log_class_probab_at_t_plus_zt\n",
      " tensor([[[0.0609, 0.9291, 0.9235, 0.2702],\n",
      "         [0.3193, 0.5586, 0.1668, 0.7046],\n",
      "         [0.3193, 0.5586, 0.1668, 0.7046],\n",
      "         [0.2979, 0.1941, 0.9765, 0.2255],\n",
      "         [0.2979, 0.1941, 0.9765, 0.2255]]])\n"
     ]
    }
   ],
   "source": [
    "# craete a torch tensor of size (batchsize, sequencelength, nclasses) with random values\n",
    "batchsize = 1\n",
    "sequencelength = 5\n",
    "nclasses = 4\n",
    "\n",
    "log_class_probabilities = torch.rand((batchsize, sequencelength, nclasses))\n",
    "\n",
    "# create a tensor of size (batchsize, sequencelength) with random values between 0 and 364\n",
    "timestamps_left = torch.randint(0, sequencelength-1, (batchsize, sequencelength))\n",
    "\n",
    "\n",
    "print(\"log_class_probabilities\\n\", log_class_probabilities)\n",
    "print(\"timestamps_left\\n\", timestamps_left)\n",
    "\n",
    "log_class_probab_at_t_plus_zt = log_class_prob_at_t_plus_zt(log_class_probabilities, timestamps_left)\n",
    "print(\"log_class_probab_at_t_plus_zt\\n\", log_class_probab_at_t_plus_zt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.alpha = 0.6\n",
    "        self.backbonemodel = \"LSTM\"\n",
    "        self.batchsize = 256\n",
    "        self.corrected = True\n",
    "        self.dataroot = os.path.join(os.environ.get(\"HOME\", os.environ.get(\"USERPROFILE\")),\"elects_data\")\n",
    "        self.dataset = \"breizhcrops\"\n",
    "        self.device = \"cuda\"\n",
    "        self.epochs = 100\n",
    "        self.epsilon = 10\n",
    "        self.extra_padding_list = [0]\n",
    "        self.hidden_dims = 64\n",
    "        self.learning_rate = 0.001\n",
    "        self.loss_weight = \"balanced\"\n",
    "        self.patience = 30\n",
    "        self.resume = False\n",
    "        self.sequencelength = 365\n",
    "        self.validation_set = \"valid\"\n",
    "        self.weight_decay = 0\n",
    "        self.daily_timestamps = True\n",
    "        self.original_time_serie_lengths = [102]\n",
    "        self.loss = \"daily_reward_loss\"\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2559635960 2559635960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 67523/67523 [00:31<00:00, 2157.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253658856 2253658856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data into RAM: 100%|██████████| 85310/85310 [00:33<00:00, 2554.71it/s]\n"
     ]
    }
   ],
   "source": [
    "dataroot = os.path.join(config.dataroot,\"breizhcrops\")\n",
    "input_dim = 13\n",
    "test_ds = BreizhCrops(root=dataroot,partition=config.validation_set, sequencelength=config.sequencelength, corrected=config.corrected, daily_timestamps=config.daily_timestamps, original_time_serie_lengths=config.original_time_serie_lengths)\n",
    "train_ds = BreizhCrops(root=dataroot,partition=\"train\", sequencelength=config.sequencelength, corrected=config.corrected, daily_timestamps=config.daily_timestamps, original_time_serie_lengths=config.original_time_serie_lengths)\n",
    "nclasses = train_ds.nclasses\n",
    "class_names = train_ds.labels_names\n",
    "traindataloader = DataLoader(train_ds,batch_size=config.batchsize)\n",
    "testdataloader = DataLoader(test_ds, batch_size=config.batchsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost function: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DailyEarlyRNN(config.backbonemodel, nclasses=nclasses, input_dim=input_dim, sequencelength=config.sequencelength, hidden_dims=config.hidden_dims).to(config.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude decision head linear bias from weight decay\n",
    "decay, no_decay = list(), list()\n",
    "for name, param in model.named_parameters():\n",
    "    if name == \"stopping_decision_head.projection.0.bias\":\n",
    "        no_decay.append(param)\n",
    "    else:\n",
    "        decay.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([{'params': no_decay, 'weight_decay': 0, \"lr\": config.learning_rate}, {'params': decay}],\n",
    "                                lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.loss_weight == \"balanced\":\n",
    "    class_weights = train_ds.get_class_weights().to(config.device)\n",
    "else: \n",
    "    class_weights = None\n",
    "\n",
    "if config.loss == \"early_reward\":\n",
    "    criterion = EarlyRewardLoss(alpha=config.alpha, epsilon=config.epsilon, weight=class_weights)\n",
    "elif config.loss == \"stopping_time_proximity\":\n",
    "    criterion = StoppingTimeProximityLoss(alphas=config.alphas, weight=class_weights)\n",
    "elif config.loss == \"daily_reward_loss\":\n",
    "    criterion = DailyRewardLoss(alpha=config.alpha, weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:53<1:28:23, 53.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_epoch, config\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m----> 6\u001b[0m         trainloss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraindataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         testloss, stats \u001b[38;5;241m=\u001b[39m test_epoch(model, testdataloader, criterion, config\u001b[38;5;241m.\u001b[39mdevice, return_id\u001b[38;5;241m=\u001b[39mtest_ds\u001b[38;5;241m.\u001b[39mreturn_id, daily_timestamps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdaily_timestamps)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# statistic logging and visualization...\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\coding_test\\..\\utils\\helpers_training.py:70\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device, extra_padding_list)\u001b[0m\n\u001b[0;32m     67\u001b[0m dict_padding \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_padding\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_padding}\n\u001b[0;32m     68\u001b[0m log_class_probabilities, stopping_criteria \u001b[38;5;241m=\u001b[39m model(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdict_padding)\n\u001b[1;32m---> 70\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_class_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loss\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     73\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anyam\\Desktop\\Master_thesis\\Code\\elects\\coding_test\\..\\utils\\losses\\daily_reward_loss.py:17\u001b[0m, in \u001b[0;36mDailyRewardLoss.forward\u001b[1;34m(self, log_class_probabilities, timestamps_left, y_true, return_stats)\u001b[0m\n\u001b[0;32m     13\u001b[0m N, T, C \u001b[38;5;241m=\u001b[39m log_class_probabilities\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# equation 4, right term\u001b[39;00m\n\u001b[0;32m     16\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(N, T, device\u001b[38;5;241m=\u001b[39mlog_class_probabilities\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m---> 17\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_class_probabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m log_class_probabilities_at_t_plus_zt \u001b[38;5;241m=\u001b[39m log_class_prob_at_t_plus_zt(log_class_probabilities, timestamps_left)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# earliness reward \u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------- TRAINING -----------------------------\n",
    "start_epoch = 1\n",
    "print(\"starting training...\")\n",
    "with tqdm(range(start_epoch, config.epochs + 1)) as pbar:\n",
    "    for epoch in pbar:\n",
    "        trainloss = train_epoch(model, traindataloader, optimizer, criterion, device=config.device)\n",
    "        testloss, stats = test_epoch(model, testdataloader, criterion, config.device, return_id=test_ds.return_id, daily_timestamps=config.daily_timestamps)\n",
    "\n",
    "        # statistic logging and visualization...\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0], average=\"macro\",\n",
    "            zero_division=0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(\n",
    "            y_pred=stats[\"predictions_at_t_stop\"][:, 0], y_true=stats[\"targets\"][:, 0])\n",
    "        kappa = sklearn.metrics.cohen_kappa_score(\n",
    "            stats[\"predictions_at_t_stop\"][:, 0], stats[\"targets\"][:, 0])\n",
    "\n",
    "        classification_loss = stats[\"classification_loss\"].mean()\n",
    "        earliness_reward = stats[\"earliness_reward\"].mean()\n",
    "        earliness = 1 - (stats[\"t_stop\"].mean() / (config.sequencelength - 1))\n",
    "        harmonic_mean = harmonic_mean_score(accuracy, stats[\"classification_earliness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
